<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Liamming</title>
  <subtitle>Ming 的碎碎念</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://liamlee.github.io/"/>
  <updated>2017-02-14T11:52:33.000Z</updated>
  <id>https://liamlee.github.io/</id>
  
  <author>
    <name>Ming</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>斯坦福(吴恩达)-机器学习-第三周编程作业代码笔记</title>
    <link href="https://liamlee.github.io/2017/02/09/logisticRegression/"/>
    <id>https://liamlee.github.io/2017/02/09/logisticRegression/</id>
    <published>2017-02-09T10:38:27.000Z</published>
    <updated>2017-02-14T11:52:33.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="假设函数"><a href="#假设函数" class="headerlink" title="假设函数"></a>假设函数</h3><p>先把假设函数放出来，一看假设函数，明白的人就已基本明白是个怎么回事了。<br>$$<br>h_\theta(x) = g(\theta^Tx)<br>$$<br>其中\(g\)：<br>$$<br>g(z) = \frac{1} { 1 + e^{-z} }<br>$$<br>\(g\)就是所谓的sigmoid函数。那么首先解决sigmoid函数的代码。</p>
<h3 id="简单模型"><a href="#简单模型" class="headerlink" title="简单模型"></a>简单模型</h3><h4 id="Sigmoid函数"><a href="#Sigmoid函数" class="headerlink" title="Sigmoid函数"></a>Sigmoid函数</h4><p>代码很简单</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">function g = sigmoid(z)</div><div class="line">%SIGMOID Compute sigmoid function</div><div class="line">%   g = SIGMOID(z) computes the sigmoid of z.</div><div class="line"></div><div class="line">% You need to return the following variables correctly </div><div class="line">g = zeros(size(z));</div><div class="line"></div><div class="line">% ====================== YOUR CODE HERE ======================</div><div class="line">% Instructions: Compute the sigmoid of each value of z (z can be a matrix,</div><div class="line">%               vector or scalar).</div><div class="line">g = 1 ./ (1 + exp(-z));</div><div class="line">% =============================================================</div><div class="line">end</div></pre></td></tr></table></figure>
<h4 id="CostFunction"><a href="#CostFunction" class="headerlink" title="CostFunction"></a>CostFunction</h4><p>LogisticRegression问题同LinearRegression问题解决思路类似：构造costFunction，然后利用梯度下降算法求解min{costFunction}以此来求得合适的\(\theta\)。</p>
<p>不同之处在于，LogisticRegression的costFunction不再使用“最小二乘法”，而是利用概率模型思想，利用样本来倒推出样本出现的最大概率，即极大似然估计思想。不过，进一步深究就会发现其实“极大似然”包含着“最小二乘法”，为什么暂且不表。于是有：</p>
<p>\(J(\theta) = \frac{1}{m}\sum<em>{i = 1}^m[-y^{(i)}\log(h</em>\theta(x^{(i)})) - (1 - y^{(i)})\log(1 - h_\theta(x^{(i)}))]\)</p>
<p>而梯度函数：<br>$$<br>\frac{\partial J(\theta)}{\partial\theta<em>j} = \frac{1}{m}\sum</em>{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)}<br>$$<br>代码表示</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line">function [J, grad] = costFunction(theta, X, y)</div><div class="line">%COSTFUNCTION Compute cost and gradient for logistic regression</div><div class="line">%   J = COSTFUNCTION(theta, X, y) computes the cost of using theta as the</div><div class="line">%   parameter for logistic regression and the gradient of the cost</div><div class="line">%   w.r.t. to the parameters.</div><div class="line"></div><div class="line">% Initialize some useful values</div><div class="line">m = length(y); % number of training examples</div><div class="line"></div><div class="line">% You need to return the following variables correctly </div><div class="line">J = 0;</div><div class="line">grad = zeros(size(theta));</div><div class="line"></div><div class="line">% ====================== YOUR CODE HERE ======================</div><div class="line">% Instructions: Compute the cost of a particular choice of theta.</div><div class="line">%               You should set J to the cost.</div><div class="line">%               Compute the partial derivatives and set grad to the partial</div><div class="line">%               derivatives of the cost w.r.t. each parameter in theta</div><div class="line">%</div><div class="line">% Note: grad should have the same dimensions as theta</div><div class="line">%</div><div class="line">h = sigmoid(X * theta);</div><div class="line">J = (-y&apos; * log(h) - (1 - y)&apos; * log(1 - h)) / m;</div><div class="line">grad = ((h - y)&apos; * X) / m;</div><div class="line">% =============================================================</div><div class="line">end</div></pre></td></tr></table></figure>
<h4 id="应用-分类"><a href="#应用-分类" class="headerlink" title="应用-分类"></a>应用-分类</h4><p>利用梯度下降算法求得合适的\(\theta\)后，也就得到的模型，就可以应用于test数据的分类了。当然，先得把分类方法实现：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">function p = predict(theta, X)</div><div class="line">%PREDICT Predict whether the label is 0 or 1 using learned logistic </div><div class="line">%regression parameters theta</div><div class="line">%   p = PREDICT(theta, X) computes the predictions for X using a </div><div class="line">%   threshold at 0.5 (i.e., if sigmoid(theta&apos;*x) &gt;= 0.5, predict 1)</div><div class="line"></div><div class="line">m = size(X, 1); % Number of training examples</div><div class="line"></div><div class="line">% You need to return the following variables correctly</div><div class="line">p = zeros(m, 1);</div><div class="line"></div><div class="line">% ====================== YOUR CODE HERE ======================</div><div class="line">% Instructions: Complete the following code to make predictions using</div><div class="line">%               your learned logistic regression parameters. </div><div class="line">%               You should set p to a vector of 0&apos;s and 1&apos;s</div><div class="line">%</div><div class="line"></div><div class="line">p = round(sigmoid(X * theta));</div><div class="line"></div><div class="line">% =========================================================================</div><div class="line"></div><div class="line">end</div></pre></td></tr></table></figure>
<p>到此，一个简单的LogisticRegression模型就实现了。</p>
<h3 id="加入正则化-让模型复杂更复杂一点"><a href="#加入正则化-让模型复杂更复杂一点" class="headerlink" title="加入正则化-让模型复杂更复杂一点"></a>加入正则化-让模型复杂更复杂一点</h3><p>当数据分布比较复杂时，比如无法用简单的一条直线将数据分类开来。本练习中介绍一种构造更多的特征的方法，通过对原特征数据进行映射得到新的特征来训练模型，更多维的特征数据可以让模型训练得到更复杂的决策分界线，以此来达到将数据最优分类的目的。</p>
<p>随着特征维度变高变复杂，我们将训练得到更复杂的模型，而越复杂的模型会越可能出现对数据过拟合的现象，引入正则化正是为了克服过拟合的问题。</p>
<p>额，这一段是我自己编的，准不准确看您的功力来分辨了。</p>
<p>本练习重点关注正则化，至于特征构造课程已给出实现代码供于参考。</p>
<h4 id="新的CostFunction"><a href="#新的CostFunction" class="headerlink" title="新的CostFunction"></a>新的CostFunction</h4><p>$$<br>J(\theta) = \frac{1}{m}\sum<em>{i = 1}^m[-y^{(i)}\log(h</em>\theta(x^{(i)})) - (1 - y^{(i)})\log(1 - h<em>\theta(x^{(i)}))] + \frac{\lambda}{2m}\sum</em>{j=1}^n\theta_j^2<br>$$</p>
<p>对应新的梯度函数<br>$$<br>\frac{\partial J(\theta)}{\partial \theta<em>0} = \frac{1}{m}\sum</em>{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)}  \ \ \ \ \ \ \ \ \textrm{for $j = $0}<br>$$</p>
<p>$$<br>\frac{\partial J(\theta)}{\partial \theta<em>j} = (\frac{1}{m}\sum</em>{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)}) + \frac{\lambda}{m}\theta_j  \ \ \ \ \ \ \ \ \textrm{for $j &gt;= $1}<br>$$</p>
<p>这里需要特别注意的是，\(\theta_0\)是不需要做正则化处理的，所以代码实现上也得注意了：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line">function [J, grad] = costFunctionReg(theta, X, y, lambda)</div><div class="line">%COSTFUNCTIONREG Compute cost and gradient for logistic regression with regularization</div><div class="line">%   J = COSTFUNCTIONREG(theta, X, y, lambda) computes the cost of using</div><div class="line">%   theta as the parameter for regularized logistic regression and the</div><div class="line">%   gradient of the cost w.r.t. to the parameters. </div><div class="line"></div><div class="line">% Initialize some useful values</div><div class="line">m = length(y); % number of training examples</div><div class="line"></div><div class="line">% You need to return the following variables correctly </div><div class="line">J = 0;</div><div class="line">grad = zeros(size(theta));</div><div class="line"></div><div class="line">% ====================== YOUR CODE HERE ======================</div><div class="line">% Instructions: Compute the cost of a particular choice of theta.</div><div class="line">%               You should set J to the cost.</div><div class="line">%               Compute the partial derivatives and set grad to the partial</div><div class="line">%               derivatives of the cost w.r.t. each parameter in theta</div><div class="line"></div><div class="line">h = sigmoid(X * theta);</div><div class="line">J = (-y' * log(h) - (1 - y)' * log(1 - h)) / m + lambda / 2 / m * (theta' * theta - (theta(1,1))^2);</div><div class="line"></div><div class="line">%Note that you should not regularize the parameter θ0</div><div class="line">grad(1,1) = ((h - y)' * X(:,1)) / m;</div><div class="line"></div><div class="line">n = size(theta);</div><div class="line">% 不是很熟悉matlab，所以用个循环来实现，自我感觉跟清晰易懂一些。</div><div class="line">for i = 2:n</div><div class="line">  grad(i) = ((h - y)' * X(:,i)) / m + lambda / m * theta(i);</div><div class="line">end</div><div class="line">% =============================================================</div><div class="line"></div><div class="line">end</div></pre></td></tr></table></figure>
<p>好了，不管代码美不美，又是一次100分。</p>
<blockquote>
<p>注：本章练习中，利用了Octave/MATLAB内置的fminunc函数来实现costFunction最小化求解，所以没有再做梯度下降算法的实现。</p>
</blockquote>
<p>​<br>​<br>​    </p>
]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;假设函数&quot;&gt;&lt;a href=&quot;#假设函数&quot; class=&quot;headerlink&quot; title=&quot;假设函数&quot;&gt;&lt;/a&gt;假设函数&lt;/h3&gt;&lt;p&gt;先把假设函数放出来，一看假设函数，明白的人就已基本明白是个怎么回事了。&lt;br&gt;$$&lt;br&gt;h_\theta(x) = g(\
    
    </summary>
    
    
      <category term="机器学习" scheme="https://liamlee.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="LogisticRegression" scheme="https://liamlee.github.io/tags/LogisticRegression/"/>
    
  </entry>
  
  <entry>
    <title>第一章 使用神经网络识别手写体数字</title>
    <link href="https://liamlee.github.io/2017/01/20/Chapter1_UsingNeuralNetsToRecognizeHandwrittenDigits/"/>
    <id>https://liamlee.github.io/2017/01/20/Chapter1_UsingNeuralNetsToRecognizeHandwrittenDigits/</id>
    <published>2017-01-20T09:38:27.000Z</published>
    <updated>2017-02-14T03:25:56.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote>
<p>说明：本系列文章不是原创，</p>
<p>而是在学习<a href="http://neuralnetworksanddeeplearning.com/index.html" target="_blank" rel="external">Neural Networks and Deep Learning</a> 的时候顺便做的翻译。</p>
<p>正值春节假期，没有工作上的事劳心劳神。</p>
<p>拜年、聚会的觥筹交错之余，正好为自己充充电。</p>
<p>翻译这些文章也是心血来潮的决定，只是想为什么不通过边学边译来加深学习效果呢？</p>
<p>给自己一个挑战也是极好的。</p>
</blockquote>
<h2 id="引子"><a href="#引子" class="headerlink" title="引子"></a>引子</h2><p>人类的视觉系统可谓是一个世界奇迹。观察下面的手写体数字序列：</p>
<p><img src="http://neuralnetworksanddeeplearning.com/images/digits.png" alt=""></p>
<p>大多数人可以毫不费力的识别出这串数字是“504192”。这种“毫不费力”是表面性的。人类大脑两个半球都有一个基本的视觉皮层，我们也称之为\(V_1\)，它们包含了14亿个神经元，这些神经元构成了上百亿的连接。但是人类的视觉不仅仅只涉及\(V_1\)，而是由整个视觉皮质完成——\(V_2，V_3，V_4，V_5\)——每个层级依次完成更复杂的图像处理。我们就像携带了一个超级计算机在我们的大脑，经过数百万年的进化，可以非常好的理解这个视觉性的世界。识别手写体数字其实并不容易。相反的，我们人类对我们眼中的世界表现出惊人的认知能力。但这些都是在无意识中做到的，所以我们很少意识到我们的视觉系统在这个过程中完成了多么复杂的工作。</p>
<p>如果你试图编写一段程序让计算机来识别上面的数字，你就明显体会到其中的困难。看上去我们自己很容易做到的事情在计算机那里瞬间变得异常的困难。人类很简单的识别形状的直觉——一个9是上面一个圈，右下方一条竖直线——如果用算法来表示则非常困难。当你想精确的定义这种规则时，你会很快迷失在各种异常情况、特殊情况中，很难理出头绪来。</p>
<p>神经网络通过另一种途径来解决这个问题。它的核心思想是，收集大量的手写体样本，通常称之为训练样本，</p>
<p><img src="http://neuralnetworksanddeeplearning.com/images/mnist_100_digits.png" alt=""></p>
<p>然后开发一个能从这些样本进行学习的系统。换句话说，神经网络利用训练样本自动的推断识别手写体数字的规则。此外，神经网络的识别准确率可以随着样本的增多而提高。我在上面的样例图中只展示了100个训练数字样本，或许我们可以使用数以千计或者数百万甚至上亿的训练样本来建立更好的书写体识别系统。</p>
<p>本章我们将编程实现一个能够通过学习去识别手写体数字的神经网络。这个程序将只有74行代码，不适用任何专门的神经网络库。在没有人工干预的情况下，这个简单的程序的准确率将不低于96%。此外，我们会在接下来的章节中探索如何将准确率提升到99%以上。事实上，最好的商业神经网络已经有非常好的表现，银行用它们来做流程检查、邮局利用它们来做地址识别。</p>
<p>我们聚焦于手写体识别，是因为这是一个很典型的问题，对我们学习神经网络来说算得上一个绝佳的原型。原因有两点：首先，具有挑战性。手写体数字的识别不是小菜一碟的问题，同时也没有那么的难，它不要求非常复杂的方法或者大规模的计算能力。其次，我们可以通过它来探索更先进的技术，比如深度学习。所以整本书中，我们会反复的聚焦到手写体识别问题上。在本书后续章节中，我们将讨论如何将这种思想应用到其他领域，比如计算机视觉、自然语言处理等等。</p>
<p>当然，如果本章只是要编写一个能够识别手写体数字的程序，那么本章节将会很简短。但是，我们会在这个过程中探索很多关于神经网络的关键思想，包括两种重要的人工神经元(感知器和sigmoid神经元)，以及标准的神经网络算法，即随机梯度下降。从始至终，我聚焦于揭示事物(神经网络)的本质，帮助你建立你的神经网络思维。这就需要一个漫长的讨论过程，而不是仅仅简单的罗列基本原理和概念。为了让书本前的你有更深的认识，非常值得多费一些笔墨。当达到本书的最后时，我们将知道什么是深度学习，以及深度学习为什么如此的重要。</p>
<h2 id="感知器"><a href="#感知器" class="headerlink" title="感知器"></a>感知器</h2><p>什么是神经网络？首先，让我来阐释一种人工神经元：感知器。感知器最早由科学家<a href="http://en.wikipedia.org/wiki/Frank_Rosenblatt" target="_blank" rel="external">Frank Rosenblatt</a> 受到<a href="http://en.wikipedia.org/wiki/Warren_McCulloch" target="_blank" rel="external">Warren McCulloch</a> 和 <a href="http://en.wikipedia.org/wiki/Walter_Pitts" target="_blank" rel="external">Walter Pitts</a> 的早期<a href="http://scholar.google.ca/scholar?cluster=4035975255085082870" target="_blank" rel="external">工作</a> 的启发，在1950~1960间开发出来。在今天，另一种人工神经元“sigmoid”神经元更广泛应用于现代的神经网络中，也是本书重点要介绍的。虽然我们不用感知器这种神经元，但是为了更好的理解“sigmoid”神经元为什么如此设计，我们还是先花点时间了解一下感知器。</p>
<p>那么，感知器是如何工作的呢？一个感知器接收几个二进制输入，\(x_1, x_2, \ldots\)，然后输出一个的二进制结果：</p>
<p><img src="http://neuralnetworksanddeeplearning.com/images/tikz0.png" alt=""></p>
<p>在上面的图例中，感知器有三个输入, \(x_1,x_2,x_3\)。通常它可以有更少或者更多的输入。Rosenblatt提出一个简单的计算输出的规则。他引入了权重，\(w_1,w_2,\ldots\)，用实数来表示各个输入对输出的重要程度。神经元的输出，0或者1，由输入的加权和值\(\sum_j w_j x_j\)是否大于某个阀值来决定。跟权重一样，阀值同样也是神经元的一个实数参数。用精确的数学表达式表示：<br>$$<br>\begin{eqnarray}<br>  \mbox{output} &amp; = &amp; \left{ \begin{array}{ll}<br>      0 &amp; \mbox{if } \sum_j w_j x_j \leq \mbox{ threshold} \<br>      1 &amp; \mbox{if } \sum_j w_j x_j &gt; \mbox{ threshold}<br>      \end{array} \right.<br>\tag{1}\end{eqnarray}<br>$$<br>这就是感知器的工作方式！</p>
<p>以上是基本的数学模型。你可以这么来看待一个感知器：通过对证据进行加权然后做出决策的一种设备。让我来举个不是很实际但是很容易理解的例子，紧接着我们会看到一些更实际的例子。假设周末临近，你得到消息你所在的城市将要举行一场奶酪节。你非常喜欢奶酪，正愁要不要去参加。你可能要权衡一下以下因素然后决定去还是不去：</p>
<ol>
<li>当天天气是不是够好？</li>
<li>你的男(女)朋友会跟你一起去吗？</li>
<li>现场离公交站远不远(你没有车)？</li>
</ol>
<p>我们用相应的二进制变量\(x_1,x_2,x_3\)表示这三个因素。例如，\(x_1 = 1\)表示好天气，\(x_1 = 0\)表示坏天气；类似的，\(x_2 = 1\)表示你的男/女朋友愿意去，\(x_2 = 0\)表示不愿意，同样的\(x_3\)来表示交通。</p>
<p>现在，假设你是奶酪的忠实粉丝，不管你的男(女)朋友是否有兴趣并且到那的交通非常不方便，你都很乐意前往。但是你是真的很难忍受坏天气，如果天气不好你说什么也不愿意去了。这时候你可以用感知器来建模做判断。一种可行的方式，选择\(w_1 = 6\)代表天气权重，\(w_2 = 2,w_3 = 2\)代表其他两项的权重。\(w_1\)的值越大说明你越重视天气因素，远远重于交通和男(女)朋友的态度。最后，假设你设定阀值5。根据这些选择，感知器变成了一个这样的需求决策模型，当天气好时模型输出1，天气不好的时候输出0，交通状况以及男(女)朋友的态度变得无关紧要。</p>
<p>通过改变权重和阀值，我们可以得到不同的决策模型。例如，假设将阀值设定为3，感知器会在天气好或者交通便利并且你男(女)朋友愿意陪你去的情况下告知你应该前往。换句话说，这是一个完全不一样的决策器。下调阀值意味着你前往的意愿在变大。</p>
<p>显然，感知器不是一个完全人性化的决策模型。但是它演示了感知器是如何通过加权证据并做出决策。我们似乎可以想象，一个复杂的由感知器组成的网络可以做一些相当微妙的决策：</p>
<p><img src="http://neuralnetworksanddeeplearning.com/images/tikz1.png" alt=""></p>
<p>在这个网络中，第一栏的感知器——通常称之为第一层感知器——完成三个很简单的决策，对输入信息进行加权。那么第二层的感知器呢？这一层的每个感知器对第一层的结果进行再次加权。如此，相对于第一层的感知器，第二层可以完成更复杂和更抽象的决策，然后再进一步复杂的决策由第三层感知器完成。这样一来，一个多层的感知器网络可以胜任复杂的决策。</p>
<p>顺带的，我在定义感知器的时候有说过，感知器只有一个输出。上面图片显示的网络中看上去是多个输出。实际上，它们仍然是一个输出，多个输出箭头表示的是一个输出被后续多个感知器当做输入。相比起来，画一个单个输出然后在拆分显得不简洁。</p>
<p>让我们来简化一下感知器的描述，条件表达式\(\sum_jw_j x_j &gt; \mbox{threshold}\) 显得繁琐，我们替换两个符号来达到简化的效果。首先，用点积替代\(\sum_jw_j x_j \)，\(w \cdot x \equiv \sum_j w_j x_j\)，\(w\),\(x\)分别表示权重和输入的向量。其次，将阀值移到不等式的左边来，用偏差概念替换原来的阀值概念：\(b \equiv -\mbox{threshold}\)。最后，感知器的规则表达式更新为：<br>$$<br>\begin{eqnarray}<br>  \mbox{output} = \left{<br>    \begin{array}{ll}<br>      0 &amp; \mbox{if } w\cdot x + b \leq 0 \<br>      1 &amp; \mbox{if } w\cdot x + b &gt; 0<br>    \end{array}<br>  \right.<br>\tag{2}\end{eqnarray}<br>$$<br>你可以把偏差理解成，感知器输出1的容易程度，或者使用更偏生物的术语，偏差是衡量感知器激活的容易度。当感知器拥有很大的偏差时，可以很轻易的得到1的输出；而当感知器的偏差趋向负无穷时，就很难得到1的输出了。显然，偏差的引入对感知器的描述来说只是一个小小的变化，但是后续我们可以感受到其带来大大的简化效果。所以，本书以后的内容中我们将不再使用阀值而使用偏差。</p>
<p>我已经描述了感知器如何通过了对证据加权来做决策。另外，感知器可以用于计算初级逻辑函数，比如我们通常认为的底层计算，“与”、“或”，“与非”。举个例子，假设我们有一个两输入感知器，权重皆为-2以及全局偏差3。如图：</p>
<p><img src="http://neuralnetworksanddeeplearning.com/images/tikz2.png" alt=""></p>
<p>于是，我们可以看到，输入00得到输出1(因为(−2)∗0+(−2)∗0+3=3(−2)∗0+(−2)∗0+3=3 为正，这里的*表示乘法运算)。同样的，输入01和10得到1，但是输入11则得到0((−2)∗1+(−2)∗1+3=−1(−2)∗1+(−2)∗1+3=−1 为负)。那么，我们的感知器变就成了一个“与非门”。</p>
<p>“与非”的例子显示我们可以利用感知器来计算简单的逻辑函数。实际上，我们可以使用感知器网络来计算任意的逻辑函数。原因在于“与非门”对计算是通用的，所以基于“与非门”可以实现任意的计算。例如，我们可以用与非门实现\(x_1,x_2\)求和的电路。这需要计算按位和，\(x_1 \oplus x_2\)，以及进位(当\(x_1,x_2\)都为1时进位为1)，进位即是按位积\(x_1x_2\)：</p>
<p><img src="http://neuralnetworksanddeeplearning.com/images/tikz3.png" alt=""></p>
<p>用权重为-2的二输入、全局偏差为3 的感知器替换与非门，就得到一个等价的感知器网络。下图就是该网络。注意到，为了方便的画出连线箭头，我把右下角的感知器稍稍移动了一下：</p>
<p><img src="http://neuralnetworksanddeeplearning.com/images/tikz4.png" alt=""></p>
<p>值得注意的一个地方是，最左边感知器的输出被最下面的感知器当做输入使用了两次。我在定义感知器的时候，并没有限定“双重输出到一个地方”是否可行。事实上，这问题不大，如果你要严格的限制这种情况，我们用一条权重为-4的连接线替换原来的两条权重为-2的连接线即可(如果对你来说不够明显，你可以稍微花点时间推算一下)。变换后的网络图如下(其他默认权重为-2，特别的将-4做了标记)：</p>
<p><img src="http://neuralnetworksanddeeplearning.com/images/tikz5.png" alt=""></p>
<p>到目前为止，我一直把输入变量\(x1,x2\)游离的画在最左边。通常情况会额外的加一层感知器(输入层)来对输入进行编码：</p>
<p><img src="http://neuralnetworksanddeeplearning.com/images/tikz6.png" alt=""></p>
<p>输入层的感知器符号(如下)，只有输出而没有输入：</p>
<p><img src="http://neuralnetworksanddeeplearning.com/images/tikz7.png" alt=""></p>
<p>这是一种简化，并不代表感知器真的没有输入。为了便于理解，假设确实这些感知器没有输入，那么加权求和\(\sum_j w_j x_j\)将一直是0，于是感知器只能输出固定的值：\(b&gt;0\) 输入1；\(b&lt;0\) 输入0，而不是我们所期望的值。最好的方式就是将输入感知器看做单纯用于输出期望的值\(x_1,x_2,\ldots\)的特殊单元。</p>
<p>加法器的例子展示了感知器网络如何模拟实现包含多个与非门的电路。因为与非门是通用的计算单位，所以感知器也遵循这个原则。</p>
<p>感知器的通用性让人感到兴奋的同时也让人失望。可喜的是，因为其通用性，感知器网络可以比得上其他任何计算机设备；而失望的是，感知器不就是另一种与非门吗？有什么可兴奋的呢。</p>
<p>然而，真实情况还不至于这么让人沮丧。事实证明，我们可以设计自动适配权重和偏差的学习算法用于网络中的神经元。这种适配源于对外部刺激的反馈，而不是由程序员的直接介入。这种学习算法给我们提供了一种完全不同于常规逻辑门的方法来使用人工神经元。不同于精确地将与非门的原件布局在电路上，神经网络是通过“学习”来解决的问题，因为有些时候要直接的设计常规的电路来应对某些难题会非常的困难。</p>
<h2 id="Sigmoid神经元"><a href="#Sigmoid神经元" class="headerlink" title="Sigmoid神经元"></a>Sigmoid神经元</h2><p>学习算法听起来妙极了。问题是我们如何才能设计出适用于神经网络的学习算法呢？假设我们有一个感知器网络，并且想用它来解决一些实际问题。例如，用手写体数字的原始像素扫面数据作为输入，我们期望这个网络能通过学习获得“权重”和“偏差”以便能正确的识别出扫面数据中的数字。为了演示这种学习是如何进行的，先假设我们对权重(偏差)做一点微小的改动，我们期望的是网络的输出也随之发生一点细微的变动，这种特性使得网络在某一时刻达到学习的目标。如下示意图展示了我们的期望(当然这个网络过于简单，不足以完成手写体的识别)：</p>
<p><img src="http://neuralnetworksanddeeplearning.com/images/tikz8.png" alt=""></p>
<p>如果权重(偏差)微小的变化只引起输出细微的变化，那我们可以依此来调整权重(偏差)的得到我们期望行为的网络。例如，假设我们的网络错误的将“9”识别成了“8”，我们可以找出如何在权重(偏差)上做微调让网络分类结果更接近“9”，然后不断的重复这种微调让输出结果越来越好，最终我们的网络学会了学习。</p>
<p>问题是，当我们的网络包含感知器的时候就不是这样的了。实际情况是，网络中任意一个感知器的权重或偏差发生一点细微的变化可能导致该感知器的输出翻转即0变1。这种翻转可能引起整个网络发生复杂的变化。所以，当你通过调整能正确的识别“9”了，但是其他的图片识别效果可能变得不可控了。这样一来，想要通过逐步的细微调整来让网络慢慢接近预期行为就变得不太现实了。也许确实存在某种巧妙的方法可以解决这种问题，但现在当务之急是我们怎么才能让感知器网络能学习。</p>
<p>我们引入另一种人工神经元来克服这个难题，也就是sigmod神经元。sigmoid神经元与感知器相似，但它在权重或偏差发生细微变化时只会引起输出的细微变化，这就是sigmoid网络能学习的关键因素。</p>
<p>现在，让我来正式描述一下sigmoid神经元。我们使用与感知器相似的图例表描绘sigmoid神经元：</p>
<p><img src="http://neuralnetworksanddeeplearning.com/images/tikz9.png" alt=""></p>
<p>跟感知器一样，sigmoid神经元有输入，\(x_1,x_2,\ldots\)，但是它的输出是0到1之间的任意实数，包含0和1，而不仅仅是0和1。比如，0.638…对sigmoid神经元来说是一个合法的输出。同样的，sigmoid神经元的每个输入也有权重，\(w_1,w_2,\ldots\)，以及全局偏差b。但是它的输出是\(\sigma(w \cdot x+b)\)，\(\sigma\)即是sigmoid函数，定义如下：<br>$$<br>\begin{eqnarray}<br>  \sigma(z) \equiv \frac{1}{1+e^{-z}}.<br>\tag{3}\end{eqnarray}<br>$$<br>我们把输入、权重以及偏差带入公式得到更精确的表达式：<br>$$<br>\begin{eqnarray}<br>  \frac{1}{1+\exp(-\sum_j w_j x_j-b)}.<br>\tag{4}\end{eqnarray}<br>$$<br>咋一看，sigmoid神经元与感知器差别很大。如果你不熟悉，sigmoid函数的代数表达式则显得晦涩难懂让人生畏。实际上，感知器与sigmoid神经元存在很多相似的地方，同时sigmoid函数的代数表达式更多的是显示了技术细节，而不是增加了理解上的障碍。</p>
<p>为了类感知器模型，假设\(z\equiv w \cdot x + b\)是一个非常大的正数，那么\(e^{-z}\approx 0\)，即\(\sigma(z) \approx 1\)。也就是说，当\(z = w\cdot x+b\)趋近于正无穷时，sigmoid神经元输出趋近于1，就像感知器。同理，当\(z = w \cdot x+b\)趋向于负无穷时，有\(e^{-z} \rightarrow \infty, \sigma(z) \approx 0\)。所以当\(z = w \cdot x +b\)趋向于负无穷时，sigmoid神经元也相当于一个感知器。仅当\(w \cdot x+b\)的值大小适中时，sigmoid神经元与感知器模型相差甚远。</p>
<p>\(\sigma\)的代数形式究竟是什么？我如何来理解它？其实，\(\sigma\)的精确形式并没有那么重要，真正重要的是函数画出来的图形，如下图：</p>
<p><img src="/img/ML/sigmoidfunction-1.png" alt=""></p>
<p>这是跃阶函数的平滑版图形：</p>
<p><img src="/img/ML/stepfunction-1.png" alt=""></p>
<p>如果\(\sigma\)变成跃阶函数，神经元的输出是0是1取决于\(w\cdot x+b\)是正还是负，那么sigmoid神经元则退化成了一个感知器。通过使用真实的\(\sigma\)函数我们就得到了一个平滑输出的感知器。实际上，真正关键的是\(\sigma\)函数的平滑度，而并非其形式的细节。平滑意味着权重和偏差的细微变化(\(\Delta w_j,\Delta b\))将引起输出的细微变化\(\Delta \mbox{output}\)。事实上，微积分告诉我们\(\Delta \mbox{output}\)约等于：<br>$$<br>\begin{eqnarray}<br>  \Delta \mbox{output} \approx \sum_j \frac{\partial \, \mbox{output}}{\partial w_j}<br>  \Delta w_j + \frac{\partial \, \mbox{output}}{\partial b} \Delta b,<br>\tag{5}\end{eqnarray}<br>$$<br>其中求和表示包括了所有的权重，\(\partial \,\mbox{output} / \partial w_j ,\partial \,\mbox{output} / \partial b\)分别为输出对于权重和偏差的偏导数。如果你不熟悉偏导数问题也不大。上面的表达式看上去很复杂，其实它揭示着很简单的东西(好消息)：\(\Delta \mbox{output}\)是关于权重和偏差变化(\(\Delta w_j,\Delta b\))的线性函数。这种线性特性让通过选择权重和偏差的细微变化得到期望的输出细微变化变得简单了。所以，sigmoid神经元在保留很多感知器属性的同时，也使得找出如何通过改变权重和方差来改变输出变得容易了很多。</p>
<p>如果真如上面说的，重要的是\(\sigma\)的形状而不是其精确形式，那我们为什么挑选的是公式(3)那样的表达式呢？实际上，本书后续内容中，我们会偶尔考虑其他形式激活函数\(f(\cdot)\)的神经元。当我们选取不同的激活函数时，主要是引起了偏导数(5)值的变化。后面会看到我们的偏导数计算由于选取了\(\sigma\)在代数上得到了很大的简化，原因在于指数函数在求导时有着很友好的特性。任何场景下，\(\sigma\)都是普遍应用于神经元中，这也是本书用到最多的激活函数。</p>
<p>我们如何解释sigmoid神经元的输出呢？显然，sigmoid神经元与感知器一个最大的区别在于sigmoid神经元的输出不仅仅是0和1，它可以输出0和1之间的任意实数，如0.173……和0.689……都是合法的输出。这是很有用的一个特性，比如，我们想用输出值来表示一个神经网络输入图像像素的平均强度。但也有麻烦的时候，假设，我们要求网络输出显示“输入图片是9”或者“输入图片不是9”。显然，像感知器那样直接输出1或0更容易。实际情况下，我们可以做一些约定来解决这个问题，如，当输出大于等于0.5时判定为“9“，小于0.5时判定不是”9“。为了不引起混淆，每次用到这种约定时我都会做明确的声明。</p>
<h3 id="练习"><a href="#练习" class="headerlink" title="练习"></a>练习</h3><ul>
<li><p><strong>Sigmoid神经元模拟感知器，一</strong></p>
<p>假设将网络中所有感知器的权重和偏差都乘以一个正的常数c。试证明网络的行为不会发生变化。</p>
</li>
<li><p><strong>Sigmoid神经元模拟感知器，二</strong></p>
<p>同样的，我们有一个感知器网络。</p>
</li>
</ul>
<h2 id="神经网络的结构"><a href="#神经网络的结构" class="headerlink" title="神经网络的结构"></a>神经网络的结构</h2><p>接下来一节中，我将介绍一个能很好的完成手写体数字分类的神经网络。我们先来了解一下定义网络中各个部分的术语。假设我们有一个如下图展示的神经网络：</p>
<p><img src="http://neuralnetworksanddeeplearning.com/images/tikz10.png" alt=""></p>
<p>如前面所提到的，神经网络中最左边的我们称之为输入层，对应的神经元则称之为输入神经元。最右边或者所谓的输出层包含了输出神经元，这里输出层只包含一个输出神经元。位于中间的，既不属于输入也不属于输出层的，我们称之为隐藏层。“隐藏”这个术语乍一听，让人有种神秘感，我头一回听到的时候以为这其中一定有很深的哲学含义或者数学思想。但实际上，它只是代表“既不是输入也不是输出”。上面展示的神经网络只有单一的隐藏层，而一些神经网络的隐藏层由多个层组成的。例如，下面的四层神经网络的隐藏层则有两层：</p>
<p><img src="http://neuralnetworksanddeeplearning.com/images/tikz11.png" alt=""></p>
<p>因为历史原因，这种多层的神经网络有时候也被称之为多层感知器或者MLPs，无论其中室友sigmoid神经元还是感知器构成。本书我不会使用MLP这个术语，以让读者困惑，这里只是特地说明有这回事，读者在以后碰到的时候能够分辨。</p>
<p>输入输出层通常设计得很简单。例如，假设我们要识别手写体的图像是否为“9”。一种通常的做法是在输入神经元对图像像素的强度进行编码，如果图像是64*64的灰度图像，则需要\(4,096 = 64 \times 64\)个输入神经元，用0到1之间合适的值表示其像素强度。输出层将只有一个神经元，当值小于0.5时判定“不是9”，当值大于0.5时判定“是9”。</p>
<p>隐藏层的设计则很具艺术性。尤其的，隐藏层的设计不是一些简单的法则可以概括的。相反的，神经网络研究人员为隐藏层开发了许多的启发式算法来帮助人们实现所期望的神经网络。如权衡隐藏层数与训练耗时的算法。本书后续会提到一些诸如此类的启发式算法。</p>
<p>截止到目前，我们讨论了前馈神经网络，即前一层的输出作为下一层的输入。神经网络中不存在环路，信息总是向前传播没有反向的。一旦引入环，则会出现\(\theta\)函数的输入将受输出的影响的情况，这样的神经网络很难变得有意义，所以目前我们不考虑这种环路。</p>
<p>然而，在其他的人工神经网络模型中反馈回路是可行的，即<a href="https://en.wikipedia.org/wiki/Recurrent_neural_network" target="_blank" rel="external">RNN</a>。其思想是让神经元在一段有限的时间里处于激活状态，这些活跃的神经元可以刺激其他神经元在稍后变得活跃，继而刺激更多的神经元，一段时间后我们的到一个瀑布式的神经元激活流程。在这种模型中，回路不会引起问题，原因在于神经元的输出是在稍后才起作用，而不是即刻。</p>
<p>循环神经网络不如前向神经元网络应用广泛，部分原因在于循环神经网络的学习算法相对比较弱一些(至少目前为止)。但是循环神经网络依然很有意思，它的思想更接近人类的大脑工作模式。用循环神经网络来解决那些前向神经网络需要花费大力气才能解决的重大问题不是没有可能。然而，本书专注于应用更广泛的前向神经网络。</p>
<h2 id="一个简易的手写体数字识别神经网络"><a href="#一个简易的手写体数字识别神经网络" class="headerlink" title="一个简易的手写体数字识别神经网络"></a>一个简易的手写体数字识别神经网络</h2><p>在定义好神经网络后，我们回到手写体识别这个问题上。我将问题拆分为两个子问题。问题一，如何将包含多个数字的图片分割成只包含一个数字的小图，比如，我们要分割下面这个图片</p>
<p><img src="http://neuralnetworksanddeeplearning.com/images/digits.png" alt=""></p>
<p>分割成6个小图片，</p>
<p><img src="http://neuralnetworksanddeeplearning.com/images/digits_separate.png" alt=""></p>
<p>人工处理这种分割问题很简单，但是计算机要想准确的对图片做分割就很有挑战性了。但完成分割后，程序就需要对每个图片做分类。举个例子，我们用程序识别第一个数字，</p>
<p><img src="http://neuralnetworksanddeeplearning.com/images/mnist_first_digit.png" alt=""></p>
<p>这是一个5。</p>
<p>我们重点关注的是实现程序来解决问题二，即对每个独立的数字图片做分类。因为一旦有一个好的数字图片分类方法，图片分割就不是什么难事了。图片分割的方法非常多。一种可行的方法，先对图像做多种实验性分割，然后利用单个数字分类器对每类分割做评分。如果单个数字分类器对某种分割的所有分片都很有信心，则该分割方式获得一个高分；相反的，如果分类器对其中一个或多个分片的分类存在问题，则该分割方式获得低分。这种方法的核心思想是分割不准确讲导致分类器表现不佳。通过这种方式或者其衍生方法可以比较好的解决图片分割问题。所以，相比起图片分割问题，我们更关注能解决更多有趣且复杂问题的神经网络，即单个手写体数字识别。</p>
<p>我们将使用一个三层的神经网络来做单个数字识别：</p>
<p><img src="http://neuralnetworksanddeeplearning.com/images/tikz12.png" alt=""></p>
<p>输入层由对输入像素进行编码的神经元组成。如前面提到的，我们的训练数据是大量的28*28像素的手写体数字扫描图片，所以输入层需要\(784 = 28\times 28\)个神经元。出于简洁考虑，上面的图表没有完全画出784个神经元。输入像素是灰度值，0.0表示白色，1.0表示黑色，介于中间的随着值递增颜色从白逐渐加深变黑色。</p>
<p>第二层即是隐藏层。我们用变量\(n\)表示该层的神经元数目，\(n\)将取不同的值做测试。例图呈现了一个小型的隐藏层，只包含\(n = 15\)个神经元。</p>
<p>输出层则包含10个神经元。如果第一个神经元激活，即其输出\(\approx 1\)，那么神经网络判定输入数字是0，如果第二个神经元激活则判定是1，以此类推。更准确一点的说，我们将输出神经元从0到9编号，然后看哪个神经元有最高的激活值。比如编号6的神经元激活值最高，那么神经网络判定输入的数字是6，其他同理。</p>
<p>或许你会问为什么我们用了10个神经元。毕竟，神经网络的目标是识别出输入图片对应的是哪个数字\((0, 1, 2, \ldots, 9)\)。更通常的做法4个神经元就够了，每个神经元可以表示两个值，根据其输出是接近0还是1。由于\(2^4 = 16\)，4个神经元足以表示10中可能性了，为什么我们还是用了10个神经元呢？这样不是效率更低吗？其实这是经验之谈，我们可以同时使用这两种设计，但最终在这个问题上，事实证明用10个输出神经元效果要好于用4个。那么问题来了，这是为什么呢？有没有什么启发式算法来告诉我们应该用10输出编码而不是用4输出编码呢？</p>
<p>为了理解为什么这么做，我们最好从本质上思考一下神经网络在做什么。考虑10个输出神经元的情形，我们只看第一个神经元，也就是通过权衡隐藏层的信息来判定是否是0的神经元。那么隐藏层做了什么事？根据输入简单的假设隐藏层中第一个神经元只是用于检测图像是否是下面这样的：</p>
<p><img src="http://neuralnetworksanddeeplearning.com/images/mnist_top_left_feature.png" alt=""></p>
<p>当输入像素与它匹配时增加其权重，对其他的输入则减轻其权重。同理，假设隐藏层中第二个、第三个、第四个神经元依次检测下列图像：</p>
<p><img src="http://neuralnetworksanddeeplearning.com/images/mnist_other_features.png" alt=""></p>
<p>那么，根据前面所看到的数字图片，你可以猜到这四个图片合起来就是一个0。</p>
<p><img src="http://neuralnetworksanddeeplearning.com/images/mnist_complete_zero.png" alt=""></p>
<p>所以，当隐藏层中这四个神经元都激活了，我们就可以得出结论，这数字是0。当然，这不是判断图像是否是0的的唯一方法，还有很多其他方法可以做到。但可以负责任的说，至少在这个例子中我们得出了输入是0的结论。</p>
<p>如果神经网络工作模式是这样的，那么我们似乎就可以解释为什么10输出网络要优于4输出网络了。如果我们只有4个输出神经元，那么第一个神经元需要判断输入图像更接近哪一个数字，但是要把上面那些简单的形状对应到某个数字就不是那么容易了。很难找出一种历史经验说明数字的局部形状与输出密切相关。</p>
<p>现在，说了这么多，都仅仅是一个引导性的。并不是说一个三层神经网络一定是像我说的那样运作的，即隐藏层的神经元只是检测简单的分量形状。或许有更聪明的算法能让我们使用4输出神经元。但是我通过这种探索性的思想可以把问题描述得很清晰，可以帮助你在设计一个好的神经网络中节省大量的时间。</p>
<h3 id="练习-1"><a href="#练习-1" class="headerlink" title="练习"></a>练习</h3><h2 id="用梯度下降进行学习"><a href="#用梯度下降进行学习" class="headerlink" title="用梯度下降进行学习"></a>用梯度下降进行学习</h2><p>好了，我们已经有了神经网络的设计了，但是如何用它来识别数字呢？首先，我们需要一个训练数据集来供神经网络学习。我们将使用<a href="http://yann.lecun.com/exdb/mnist/" target="_blank" rel="external">MNIST data set</a>，这个数据集包含了上万个已正确分类的手写体扫描图片。MNIST’s这个名字源于这个数据集是由<a href="http://en.wikipedia.org/wiki/National_Institute_of_Standards_and_Technology" target="_blank" rel="external">NIST</a>收集并整理的两个数据源。下图就是MNIST里面的一些样本图片</p>
<p><img src="http://neuralnetworksanddeeplearning.com/images/digits_separate.png" alt=""></p>
<p>你可以看出来，这些数字就是本文开篇所提及的需要识别的数字。当然，用于测试我们的神经网络的图片不包含在训练集中。</p>
<p>MNIST数据集由两部分组成。第一部分是作为训练集的60,000个图片，这些图片是250个人的手写体样本扫描所得，其中一半人员来自美国人口普查办事处的雇员，另一半则是高中生。都是28✖28像素大小的灰度图片。第二部分则是作为测试数据集的10,000个图片，同样的是28✖28的灰度图像。测试数据集供我们评估神经网络的识别效果。为了实现更好的测试效果，测试数据从不同于训练数据的另外250个人中收集(尽管依然是人口普查办事处雇员和高中生)。这让我们对系统更有信心，它可以识别没有在训练集出现的手写体数字。</p>
<p>我们将用\(x\)表示训练集输入。为了方便，将输入\(x\)看做\(28 \times 28 = 784\)维向量，向量中每个实体代表图片的一个像素值。将输对应的期望出定义为\(y = y(x)\)，其中\(y\)是一个10维向量。比如，一个输入图像，\(x\)，是6，那么\(y(x) = (0, 0, 0, 0, 0, 0, 1, 0, 0, 0)^T\)就是神经网络的期望输出。T表示转置，将一个行向量转置为一个列向量。</p>
<p>我们想要这样一个算法，就是能让我们找到能让神经网络对所有训练输入\(x\)的输出都趋近\(y(x)\)的权重和偏差。我们定义cost函数来量化评估模型的效果：<br>$$<br>\begin{eqnarray}  C(w,b) \equiv<br>  \frac{1}{2n} \sum_x | y(x) - a|^2.<br>\tag{6}\end{eqnarray}<br>$$<br>这里的\(w\)是表示神经网络的所有权重的集合，\(b\)是所有的偏差，\(n\)表示训练集的大小，\(a\)则是输入\(x\)经过神经网络后的输出，求和即表示复杂所有的输入样本，\(x\)。当然，输出\(a\)取决于\(x,w,b\)，但是现阶段为了简单我不打算解释其中的关系。\(| v |\)表示向量\(v\)的长度。我将\(C\)称之为二次cost函数；也叫最小二乘法或者MSE。根据cost函数表达式，我们可以看出\(C(w,b)\)是非负的，因为每个求和项都是非负的。此外，对所有的训练输入\(x\)，当输出\(a\)与\(y(x)\)接近时，代价函数\(C(w,b)\)变得很小，即\(C(w,b) \approx 0\)。所以，如果我们能找到使\(C(w,b) \approx 0\)的权重和偏差时，我们的训练算法将有一个很好的效果。相比之下，当\(C(w,b)\)很大时算法则表现很不好，因为这意味着\(y(x)\)与输出\(a\)相差甚远。因此，我们训练算法的目标就是求权重和偏差对应的代价函数\(C(w,b)\)最小化。换句话说，就是要找出使得代价函数尽可能小的权重和偏差。我们将使用梯度下降算法来完成这个任务。</p>
<p>那为什么又要引入二次代价函数呢？毕竟，我们的重点不是在神经网络能正确的分类多少图片吗？何不直接求这个正确数量的最大值，而是去最小化一个间接如二次代价函数的方法呢？问题关键是正确分类图片的数量不是一个关于神经网络中的权重和偏差的平滑函数。大部分情况下，对权重和偏差做细微调整并不会引起图片正确分类数量上的任何变化。如此一来，就很难知道怎么去调整权重和偏差来提升分类效果。但是如果我们换一种思路，使用一个平滑的代价函数，如二次代价函数，这类函数使得通过不断对权重和偏差做细微调整来找到提升效果的途径变得更容易。所以，我们先聚焦于二次代价函数的最小化，然后再做分类准确性评估。</p>
<p>虽然解释了为什么要使用平滑的代价函数，你可能依然会问为什么偏偏就是公式(6)那样的二次函数呢，这是一个特别指定的选择吗？如果我们换一个不一样的代价函数是不是就会得到一套完全不同的权重和偏差的最小值？有这种疑虑很正常，稍后我们还会提到二次代价函数并做一些修改。然而就目前而言，如公式(6)的二次代价函数对理解神经网络如何进行学习的基础来说很适用。</p>
<p>回顾一下，我们训练一个神经网络的目的是找到一组权重和偏差让二次代价函数\(C(w,b)\)最小化。这是一个适定性问题，但现阶段还有很多分散我们注意力的地方，比如对权重和偏差的解释、隐藏在背后的\(\delta\)函数、网络结构的选择、MNIST数据集等等。事实上我们就算忽略上面大部分东西来understand a tremendous amount(这实在不知道如何翻译)，只关注最小化方面的问题。现在，让我们忘掉代价函数的所有明确形式、与神经网络的联系等等。相反的，我们想象已经有一个多元函数，我要最小化求解这个函数。我们要开发一个人称“梯度下降”的技术来解决最小化问题，然后再返回到需要去最小化的具体函数。</p>
<p>好，假设现在我们要最小化某个函数，\(C(v)\)。可以是关于任意实数的多元函数，\(v = v_1,v_2, \ldots\)我特地用\(v\)替代了符号\(w\)和\(b\)就是强调任意性，这里我们不再局限于神经网络里的某个特定函数。方便起见，假设\(C\)是一个二元函数，分别是\(v_1,v_2)：</p>
<p><img src="http://neuralnetworksanddeeplearning.com/images/valley.png" alt=""></p>
<p>我需要找到\(C\)的全局最小。当然，上图所画的，我们一眼就能看出它的最小值。似乎我展示了一个过于简单的例子，通常情况下\(C\)会是一个更复杂的多元函数，我们是无法通过肉眼找到其最小值的。</p>
<p>另一种方法是通过微积分。先求偏导数然后再求\(C\)的极值。当函数\(C\)只有一个或少数几个变量时，可行。但是，很多变量时，这种求导就显得不现实了。在神经网络中，通常变量是非常多的，目前最大的神经网络有着由十亿级的权重和偏差构成的复杂的代价函数，微积分方式根本行不通。</p>
<p>那么，微积分不奏效。幸运的是，有一个借助类似思想的算法。</p>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;说明：本系列文章不是原创，&lt;/p&gt;
&lt;p&gt;而是在学习&lt;a href=&quot;http://neuralnetworksanddeeplearning.com/index.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Neura
    
    </summary>
    
    
      <category term="神经网络" scheme="https://liamlee.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="手写体识别" scheme="https://liamlee.github.io/tags/%E6%89%8B%E5%86%99%E4%BD%93%E8%AF%86%E5%88%AB/"/>
    
      <category term="图像识别" scheme="https://liamlee.github.io/tags/%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB/"/>
    
  </entry>
  
  <entry>
    <title>斯坦福(吴恩达)-机器学习-第二周编程作业代码笔记</title>
    <link href="https://liamlee.github.io/2017/01/16/linearRegression/"/>
    <id>https://liamlee.github.io/2017/01/16/linearRegression/</id>
    <published>2017-01-16T10:38:27.000Z</published>
    <updated>2017-02-14T13:08:35.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="假设函数"><a href="#假设函数" class="headerlink" title="假设函数"></a>假设函数</h3><p>以下编程逻辑基于此处定义的假设函数<br>$$<br>h_\theta(x) = \theta^T\vec x<br>$$</p>
<p>$$<br>h_\theta(x) = \theta_0 + \theta_1x<br>$$</p>
<h3 id="computeCost"><a href="#computeCost" class="headerlink" title="computeCost"></a>computeCost</h3><p>回归问题中，通常选用Square error function，也就是我们所说的”最小二乘法”<br>$$<br> J(\theta) = \frac{1}{2m}\sum<em>{i=1}^m(h</em>\theta(x^{(i)}) - y^{(i)})^2<br>$$</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">function J = computeCost(X, y, theta)</div><div class="line">%COMPUTECOST Compute cost for linear regression</div><div class="line">%   J = COMPUTECOST(X, y, theta) computes the cost of using theta as the</div><div class="line">%   parameter for linear regression to fit the data points in X and y</div><div class="line"></div><div class="line">% Initialize some useful values</div><div class="line">m = length(y); % number of training examples</div><div class="line"></div><div class="line">% You need to return the following variables correctly </div><div class="line">J = 0;</div><div class="line"></div><div class="line">% ====================== YOUR CODE HERE ======================</div><div class="line">% Instructions: Compute the cost of a particular choice of theta</div><div class="line">%               You should set J to the cost.</div><div class="line">% 注释掉烂代码 2017-02-08(由于不熟悉matlab，虽然实现了，但是代码太难看)</div><div class="line">%h = theta&apos; .* X ;</div><div class="line">%J = sum((h(:,1) + h(:,2) - y) .^ 2) / m / 2 ;</div><div class="line">% 注释掉烂代码 2017-02-08(由于不熟悉matlab，虽然实现了，但是代码太难看)</div><div class="line">J = sum((X * theta - y) .^ 2) / m / 2;</div><div class="line"></div><div class="line">% =========================================================================</div><div class="line"></div><div class="line">end</div></pre></td></tr></table></figure>
<h3 id="computeCost-多维"><a href="#computeCost-多维" class="headerlink" title="computeCost-多维"></a>computeCost-多维</h3><p>多维情况下，为了更直观的指导编程，我们可以对函数表达式做一下改写<br>$$<br>J(\theta) = \frac{1}{2m}(X\theta - \vec y)^T(X\theta - \vec y)<br>$$</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line">function J = computeCostMulti(X, y, theta)</div><div class="line">%COMPUTECOSTMULTI Compute cost for linear regression with multiple variables</div><div class="line">%   J = COMPUTECOSTMULTI(X, y, theta) computes the cost of using theta as the</div><div class="line">%   parameter for linear regression to fit the data points in X and y</div><div class="line"></div><div class="line">% Initialize some useful values</div><div class="line">m = length(y); % number of training examples</div><div class="line"></div><div class="line">% You need to return the following variables correctly </div><div class="line">J = 0;</div><div class="line"></div><div class="line">% ====================== YOUR CODE HERE ======================</div><div class="line">% Instructions: Compute the cost of a particular choice of theta</div><div class="line">%               You should set J to the cost.</div><div class="line">% J = 1/(2*m) * (theta * X&apos; - y)&apos;(theta * X&apos; - y)</div><div class="line">% 注释掉烂代码 2017-02-08(由于不熟悉matlab，虽然实现了，但是代码太难看)</div><div class="line">%h = theta&apos; .* X ;</div><div class="line">%s = size(h,2);</div><div class="line">%j = h(:,1);</div><div class="line">%for i = 2:s</div><div class="line">%  j = j + h(:,i);</div><div class="line">%end</div><div class="line">%j = j - y;</div><div class="line">%J = sum(sum(j&apos; * j)) / m / 2;</div><div class="line">% 注释掉烂代码 2017-02-08(由于不熟悉matlab，虽然实现了，但是代码太难看)</div><div class="line">J = sum((X * theta - y)&apos; * (X * theta - y)) / m / 2; </div><div class="line"></div><div class="line">% =========================================================================</div><div class="line">end</div></pre></td></tr></table></figure>
<h3 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h3><p>批量梯度下降迭代过程中，每次迭代同时更新所有\(\theta\)，并且使用所有训练样本<br>$$<br>\theta_j := \theta<em>j - \alpha\frac1m\sum</em>{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)}<br>$$</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line">function [theta, J_history] = gradientDescent(X, y, theta, alpha, num_iters)</div><div class="line">%GRADIENTDESCENT Performs gradient descent to learn theta</div><div class="line">%   theta = GRADIENTDESCENT(X, y, theta, alpha, num_iters) updates theta by </div><div class="line">%   taking num_iters gradient steps with learning rate alpha</div><div class="line"></div><div class="line">% Initialize some useful values</div><div class="line">m = length(y); % number of training examples</div><div class="line">J_history = zeros(num_iters, 1);</div><div class="line"></div><div class="line">for iter = 1:num_iters</div><div class="line">    % ====================== YOUR CODE HERE ======================</div><div class="line">    % Instructions: Perform a single gradient step on the parameter vector</div><div class="line">    %               theta. </div><div class="line">    %</div><div class="line">    % Hint: While debugging, it can be useful to print out the values</div><div class="line">    %       of the cost function (computeCost) and gradient here.</div><div class="line">    %</div><div class="line">    % 注释掉烂代码 2017-02-08(由于不熟悉matlab，虽然实现了，但是代码太难看)</div><div class="line">    %h = theta&apos; .* X ;</div><div class="line">    </div><div class="line">    %theta(1) = theta(1) - sum((h(:,1) + h(:,2) - y)&apos; * X(:,1)) * alpha / m;</div><div class="line">    %theta(2) = theta(2) - sum((h(:,1) + h(:,2) - y)&apos; * X(:,2)) * alpha / m;</div><div class="line">    % 注释掉烂代码 2017-02-08(由于不熟悉matlab，虽然实现了，但是代码太难看)</div><div class="line">    </div><div class="line">    h = X * theta ;</div><div class="line">    theta(1) = theta(1) - sum((h - y)&apos; * X(:,1)) * alpha / m;</div><div class="line">    theta(2) = theta(2) - sum((h - y)&apos; * X(:,2)) * alpha / m;</div><div class="line"></div><div class="line">    % ============================================================</div><div class="line"></div><div class="line">    % Save the cost J in every iteration    </div><div class="line">    J_history(iter) = computeCost(X, y, theta);</div><div class="line">end</div></pre></td></tr></table></figure>
<h3 id="梯度下降-多维"><a href="#梯度下降-多维" class="headerlink" title="梯度下降-多维"></a>梯度下降-多维</h3><p>多维与前面特殊情况一样，需要注意的在编写程序时确保能支持任意维度的求解，而不是前面固定只求\(\theta_1\)、\(\theta_2\)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div></pre></td><td class="code"><pre><div class="line">function [theta, J_history] = gradientDescentMulti(X, y, theta, alpha, num_iters)</div><div class="line">%GRADIENTDESCENTMULTI Performs gradient descent to learn theta</div><div class="line">%   theta = GRADIENTDESCENTMULTI(x, y, theta, alpha, num_iters) updates theta by</div><div class="line">%   taking num_iters gradient steps with learning rate alpha</div><div class="line"></div><div class="line">% Initialize some useful values</div><div class="line">m = length(y); % number of training examples</div><div class="line">J_history = zeros(num_iters, 1);</div><div class="line">for iter = 1:num_iters</div><div class="line"></div><div class="line">    % ====================== YOUR CODE HERE ======================</div><div class="line">    % Instructions: Perform a single gradient step on the parameter vector</div><div class="line">    %               theta. </div><div class="line">    %</div><div class="line">    % Hint: While debugging, it can be useful to print out the values</div><div class="line">    %       of the cost function (computeCostMulti) and gradient here.</div><div class="line">    %</div><div class="line">    % 注释掉烂代码 2017-02-08(由于不熟悉matlab，虽然实现了，但是代码太难看)</div><div class="line">    %h = theta&apos; .* X ;</div><div class="line">    %s = size(h,2);</div><div class="line">    %j = h(:,1);</div><div class="line">    %for i = 2:s</div><div class="line">    %  j = j + h(:,i);</div><div class="line">    %end</div><div class="line">    %j = j - y;</div><div class="line">    %s = size(theta,1);</div><div class="line">    %for i = 1:s</div><div class="line">    %  theta(i) = theta(i) - sum(j&apos; * X(:,i)) * alpha / m;</div><div class="line">    %end</div><div class="line">    % 注释掉烂代码 2017-02-08(由于不熟悉matlab，虽然实现了，但是代码太难看)</div><div class="line">    </div><div class="line">    h = X * theta;</div><div class="line">    s = size(theta,1);</div><div class="line">    for i = 1:s</div><div class="line">      theta(i) = theta(i) - sum((h - y)&apos; * X(:,i)) * alpha / m;</div><div class="line">    end</div><div class="line">    % ============================================================</div><div class="line"></div><div class="line">    % Save the cost J in every iteration    </div><div class="line">    J_history(iter) = computeCostMulti(X, y, theta);</div><div class="line">end</div></pre></td></tr></table></figure>
<h3 id="特征正则化"><a href="#特征正则化" class="headerlink" title="特征正则化"></a>特征正则化</h3><p>采用均值和标准差对特征数据做正则化</p>
<ul>
<li>求得每个维度的均值、标准差</li>
<li>每个维度减去该维度的均值，然后除以标准差</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line">function [X_norm, mu, sigma] = featureNormalize(X)</div><div class="line">%FEATURENORMALIZE Normalizes the features in X </div><div class="line">%   FEATURENORMALIZE(X) returns a normalized version of X where</div><div class="line">%   the mean value of each feature is 0 and the standard deviation</div><div class="line">%   is 1. This is often a good preprocessing step to do when</div><div class="line">%   working with learning algorithms.</div><div class="line"></div><div class="line">% You need to set these values correctly</div><div class="line">X_norm = X;</div><div class="line">mu = zeros(1, size(X, 2));</div><div class="line">sigma = zeros(1, size(X, 2));</div><div class="line"></div><div class="line">% ====================== YOUR CODE HERE ======================</div><div class="line">% Instructions: First, for each feature dimension, compute the mean</div><div class="line">%               of the feature and subtract it from the dataset,</div><div class="line">%               storing the mean value in mu. Next, compute the </div><div class="line">%               standard deviation of each feature and divide</div><div class="line">%               each feature by it&apos;s standard deviation, storing</div><div class="line">%               the standard deviation in sigma. </div><div class="line">%</div><div class="line">%               Note that X is a matrix where each column is a </div><div class="line">%               feature and each row is an example. You need </div><div class="line">%               to perform the normalization separately for </div><div class="line">%               each feature. </div><div class="line">%</div><div class="line">% Hint: You might find the &apos;mean&apos; and &apos;std&apos; functions useful.</div><div class="line">%       </div><div class="line">mu = mean(X);</div><div class="line">sigma = std(X);</div><div class="line"></div><div class="line">s = size(X,2);</div><div class="line">for iter = 1:s</div><div class="line">  X_norm(:,iter) = (X_norm(:,iter) - mu(iter)) ./ sigma(iter);</div><div class="line">end</div><div class="line">% ============================================================</div><div class="line">end</div></pre></td></tr></table></figure>
<h3 id="Normal-Equation-正规方程"><a href="#Normal-Equation-正规方程" class="headerlink" title="Normal Equation(正规方程)"></a>Normal Equation(正规方程)</h3><p>根据假设函数，我们可以通过方程转换的到Normal Equation表达式直接计算得到\(\theta\)<br>$$<br>\theta = (X^TX)^{-1}X^ T\vec y<br>$$</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">function [theta] = normalEqn(X, y)</div><div class="line">%NORMALEQN Computes the closed-form solution to linear regression </div><div class="line">%   NORMALEQN(X,y) computes the closed-form solution to linear </div><div class="line">%   regression using the normal equations.</div><div class="line"></div><div class="line">theta = zeros(size(X, 2), 1);</div><div class="line"></div><div class="line">% ====================== YOUR CODE HERE ======================</div><div class="line">% Instructions: Complete the code to compute the closed form solution</div><div class="line">%               to linear regression and put the result in theta.</div><div class="line">%</div><div class="line"></div><div class="line">% ---------------------- Sample Solution ----------------------</div><div class="line">theta = (X&apos; * X)^(-1) * X&apos; * y;</div><div class="line">% -------------------------------------------------------------</div><div class="line">% ============================================================</div><div class="line">end</div></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;假设函数&quot;&gt;&lt;a href=&quot;#假设函数&quot; class=&quot;headerlink&quot; title=&quot;假设函数&quot;&gt;&lt;/a&gt;假设函数&lt;/h3&gt;&lt;p&gt;以下编程逻辑基于此处定义的假设函数&lt;br&gt;$$&lt;br&gt;h_\theta(x) = \theta^T\vec x&lt;br&gt;$$&lt;
    
    </summary>
    
    
      <category term="机器学习" scheme="https://liamlee.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="线性回归" scheme="https://liamlee.github.io/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    
      <category term="LinearRegression" scheme="https://liamlee.github.io/tags/LinearRegression/"/>
    
  </entry>
  
  <entry>
    <title>回首2016</title>
    <link href="https://liamlee.github.io/2016/12/29/look-back-2016/"/>
    <id>https://liamlee.github.io/2016/12/29/look-back-2016/</id>
    <published>2016-12-29T15:30:13.000Z</published>
    <updated>2017-01-29T14:11:45.000Z</updated>
    
    <content type="html"><![CDATA[<p>2016又即将过去，就像每个今天变成昨天，今年的2016就要变成去年2016了。我已经忘记了2016年年初有没有定下一些目标，所以不敢说完成了什么。在这里现在只能做一点回忆，做一点总结，或许写到最后还会做一点规划，定下一些目标。</p>
<h3 id="收获"><a href="#收获" class="headerlink" title="收获"></a>收获</h3><p>收货的话，今年确实收了不少，但是收获从何说起呢？</p>
<h4 id="读书心得"><a href="#读书心得" class="headerlink" title="读书心得"></a>读书心得</h4><p>读书一定可以算得上是一个，这一年虽然没有刻意的按照一个量化的目标去读书，但确实没有停止读书，读书习惯较之前可以说是更好了一些。2016年，实实在在读完的书有那么几本。</p>
<p>《易中天中华史》系列，目前出版的有17本，从我们的祖先时代读到隋唐时代，最新的关于大宋的一本暂时还没读。十六本通读下来，首先佩服易老师致力写这么一套书，书中体现出易老师严谨的历史态度，就算有偏好的历史人物，在阐述“事实”的时候也是基于严谨的史料考据和客观的态度。</p>
<p>中学上了四年的历史课，大学一年，都没有学到一点历史观，所有为了考试背诵的历史年份、历史人物、历史事件，都在考试后忘得一干二净。</p>
<p>在阅读这一套丛书的时候，开始建立了一点点的历史思维、历史观，对待历史人物、历史事件有了新的看法和思考，这种思考也顺便融入了对现在的思考中。对比历史各个时期，现在的人在很多方面都有了巨大的进步，比如科技，比如文明，但也有些方面感觉上似乎没有多大进步，比如人们依然不知道幸福为何物，面对“哲学三问”依然茫然。</p>
<p>读了几本不错的小说，像《嫌疑人X的献身》、《钟鼓楼》。刘心武先生的《钟鼓楼》着实让我印象深刻。起先并没有看过刘老师的书，知道他是从百家讲坛，但仅仅是知道，因为他讲的“解密红楼梦”我也没看过。然而这本小说让我惊叹刘老师人物刻画的功力。将北京浓缩在一个四合院中，有血有肉、个性鲜明的小人物身上。故事没有开始更没有结尾，因为一代的故事好似讲完了，一代的故事又早已开始了。</p>
<p>另外，基本经济类的书籍，《斯坦福极简经济学》、《一本书通读10位经济学大师》。不敢说读完后就懂了经济学，但是多少有些一些经济学思维。</p>
<h4 id="工作总结"><a href="#工作总结" class="headerlink" title="工作总结"></a>工作总结</h4><h3 id="迷茫"><a href="#迷茫" class="headerlink" title="迷茫"></a>迷茫</h3><p>依然迷茫。</p>
<p>迷茫工作上的未来，也带着惶恐。看看自己会的那些，迷茫自己会了这些到底能做些什么了不得的事呢。看看那些自己不会的，惶恐下一刻自己就将被淘汰。</p>
<p>依然对生活、生命的意义迷茫，没有找到一个好的答案。生存、繁衍是生命最本质的、最本能的事情，而在这之上呢？把个人的追求放在世界观里、宇宙观里，似乎变得没有一点意义。我们追求了这么久(平生)的东西，仔细看看却看不出太多的含义。到头来总说，我愿意放弃现在的一切回到当初某一个时候。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;2016又即将过去，就像每个今天变成昨天，今年的2016就要变成去年2016了。我已经忘记了2016年年初有没有定下一些目标，所以不敢说完成了什么。在这里现在只能做一点回忆，做一点总结，或许写到最后还会做一点规划，定下一些目标。&lt;/p&gt;
&lt;h3 id=&quot;收获&quot;&gt;&lt;a hre
    
    </summary>
    
    
      <category term="总结" scheme="https://liamlee.github.io/tags/%E6%80%BB%E7%BB%93/"/>
    
  </entry>
  
  <entry>
    <title>Spark-Notebook安装与调教</title>
    <link href="https://liamlee.github.io/2016/12/28/sparkNotebook-install/"/>
    <id>https://liamlee.github.io/2016/12/28/sparkNotebook-install/</id>
    <published>2016-12-28T03:02:46.000Z</published>
    <updated>2016-12-29T11:46:15.000Z</updated>
    
    <content type="html"><![CDATA[<p>近来，忙活了一段时间的项目被宣布暂时停下来，另外一条线的工作也因为等待数据中暂时无法开展起来，忽然间感觉所有工作都戛然而止了。那么，就领着“大数据平台规划”的任务折腾一些新的东西吧。依据“可视化数据分析”的概念摸摸索索，发现了SparkNotebook。下面就是这个工具的安装配置笔记。</p>
<h3 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h3><ul>
<li>直接去<a href="http://spark-notebook.io" target="_blank" rel="external">官网</a>下载编译好的包，作者提供了非常非常详尽的下载列表，所有压缩方式、版本支持、组件支持的组合一一列在了主页面，擦亮双眼找出你理想的版本下载即可。</li>
<li>去<a href="https://github.com/andypetrella/spark-notebook/" target="_blank" rel="external">Github主页</a>下载源码，然后自行编译。看到官网那么详细的列表，我猜想要自行编译出个性化的包一定很麻烦，所以我直接选择放弃源码编译。</li>
</ul>
<h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p>Spark Notebook的安装可以说非常简单，就是一个步骤：解~压~缩~。环境变量你想配或者不配，随你大小便咯。完成解压后，就可以运行起来了。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">cd spark-notebook-0.7.0</div><div class="line">./bin/spark-notebook</div></pre></td></tr></table></figure>
<p>在浏览器输入<code>http://your-host:9001</code>，即可访问notebook。</p>
<h3 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h3><p><em>配置永远是重头戏</em></p>
<p>配置文件列表：</p>
<ul>
<li>conf/application.conf</li>
<li>conf/application.ini</li>
<li>conf/clusters</li>
<li>conf/profiles</li>
</ul>
<p>其实尴尬了，细细盘点一下，发现没有对上述配置文件动过什么手脚。所以，只剩下一下遇坑爬坑的心得了，作为一个知识积累放在这里，如顺便能对其他查阅者有所帮助，那最好不过了。</p>
<ul>
<li><p><strong>连接已有的Spark集群来做数据分析</strong></p>
<ol>
<li><p>代码方式：在创建SparkSession时直接指定集群的master。不！奏！效！</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span></div><div class="line">      .builder()</div><div class="line">      .config(<span class="string">"spark.master"</span>,<span class="string">"spark://node1:7077"</span>)</div><div class="line">      .getOrCreate()</div></pre></td></tr></table></figure>
</li>
<li><p>配置“Notebook Metadata”方式：<em>这里演示的是standalone模式</em></p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">"customSparkConf": &#123;</div><div class="line">    "spark.app.name": "Notebook",</div><div class="line">    "spark.master": "spark://node1:7077",</div><div class="line">    "spark.executor.memory": "10G",</div><div class="line">    "spark.cores.max": 10,</div><div class="line">    "spark.executor.cores": 2</div><div class="line">  &#125;</div></pre></td></tr></table></figure>
</li>
<li><p>配置conf/clusters文件，在clusters文件中，我们可以看到作者已经针对各种集群任务提交模式写了example。我们可以修改其中的“customSparkConf”做个性化修改，配置你的集群信息，具体格式与上一点一样。</p>
</li>
</ol>
</li>
<li><p><strong>连接Hadoop读取HDFS文件</strong></p>
<ol>
<li><p>代码方式：在文件路径前添加HDFS地址前缀“hdfs://servesr/dataPath”</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> testRDD = sc.textFile(<span class="string">"hdfs://node1/data/test.txt"</span>)</div></pre></td></tr></table></figure>
</li>
<li><p>配置$HADOOP_CONF_DIR。很多人对这个可能没有感觉这里有坑。我踩坑是因为环境变量的问题，我的集群没有将HADOOP_CONF_DIR配置到系统(用户)环境变量中，只是在spark-env.sh中export了HADOOP_CONF_DIR，所以Spark集群本身没有问题。</p>
<p>爬坑过程异常烦躁，官方doc就没有提这事，估计觉着这就不是事。网上找吧，基本没有相关博客，github主页中的issue栏目也没有发现谁碰到了这个问题。无奈，翻遍上面提到的几个配置文件，也没发现这个配置项。</p>
<p>最后，只有一个地方了，启动脚本(<code>bin/spark-notebook</code>)。终于找到了线索：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">declare -r app_classpath=&quot;$&#123;CLASSPATH_OVERRIDES&#125;:$&#123;YARN_CONF_DIR&#125;:$&#123;HADOOP_CONF_DIR&#125;:$&#123;EXTRA_CLASSPATH&#125;</div></pre></td></tr></table></figure>
<p>notebook在启动的时候读取了环境变量，那就好办了，在~/.bashrc中添加HADOOP_CONF_DIR变量即可。当然YARN_CONF_DIR也是同样的道理。</p>
<p>另外提一点，这里发现<em>CLASSPATH_OVERRIDES</em>这个变量，我觉得这个变量可以配置spark-notebook自己需要的CLASSPATH，比如<em>figaro</em>(这个也是折腾我好半天)，避免常规的CLASSPATH太重。</p>
</li>
</ol>
</li>
<li><p>Metadata配置，这个暂时不表，后续使用过程中，应该主要在于包依赖配置。</p>
</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;近来，忙活了一段时间的项目被宣布暂时停下来，另外一条线的工作也因为等待数据中暂时无法开展起来，忽然间感觉所有工作都戛然而止了。那么，就领着“大数据平台规划”的任务折腾一些新的东西吧。依据“可视化数据分析”的概念摸摸索索，发现了SparkNotebook。下面就是这个工具的安
    
    </summary>
    
    
      <category term="大数据" scheme="https://liamlee.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Spark" scheme="https://liamlee.github.io/tags/Spark/"/>
    
      <category term="安装教程" scheme="https://liamlee.github.io/tags/%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B/"/>
    
  </entry>
  
  <entry>
    <title>Hue 安装配置</title>
    <link href="https://liamlee.github.io/2016/12/26/Hue-Install/"/>
    <id>https://liamlee.github.io/2016/12/26/Hue-Install/</id>
    <published>2016-12-26T07:37:29.000Z</published>
    <updated>2016-12-29T11:51:57.000Z</updated>
    
    <content type="html"><![CDATA[<p>Hue</p>
<h3 id="基础依赖"><a href="#基础依赖" class="headerlink" title="基础依赖"></a>基础依赖</h3><ul>
<li>JDK</li>
<li>Maven</li>
<li>Git（用于从github下载Hue） </li>
<li>集群嘛，当然我认为你已经搭建好了</li>
<li>库包依赖：参考Hue的githu主页 <a href="https://github.com/cloudera/hue" target="_blank" rel="external">Hue</a> 所列的所有依赖，一一检查安装</li>
</ul>
<hr>
<h3 id="下载安装"><a href="#下载安装" class="headerlink" title="下载安装"></a>下载安装</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">git clone https://github.com/cloudera/hue.git (brach)</div><div class="line">cd hue</div><div class="line">make apps</div></pre></td></tr></table></figure>
<p>make过程时间会比较长，只要前面所提的依赖都已安装好，整个过程一般不会出现错误，耐心等待即可</p>
<h3 id="初步验证"><a href="#初步验证" class="headerlink" title="初步验证"></a>初步验证</h3><ul>
<li>在后台启动服务 <code>build/env/bin/hue runserver</code></li>
</ul>
<ul>
<li>访问<a href="http://localhost:8000。正常情况，前台出现登录界面。" target="_blank" rel="external">http://localhost:8000。正常情况，前台出现登录界面。</a></li>
</ul>
<ul>
<li>Hue初始界面没有用户注册选项，直接根据用户的的输入创建第一个超级用户，后续建立用户分配权限都依赖该用户，所以需要特别注意，务必记住自己输入了什么。</li>
</ul>
<hr>
<h3 id="配置Hue"><a href="#配置Hue" class="headerlink" title="配置Hue"></a>配置Hue</h3><p>在完成安装和初步验证后，现在进入最重要也是最复杂的步骤，配置。Hue支持Hadoop生态系统中的各种组建，比如访问HDFS文件、执行SQL查询HBase中的表、编写notebook来运行spark，前提是要做好对应的配置。</p>
<p><strong>配置文件</strong><br>从github下载的开发版，配置文件为 <strong><em>desktop/conf/pseudo-distributed.ini</em></strong> ,如果是其他发行版请参考 <a href="http://gethue.com/how-to-configure-hue-in-your-hadoop-cluster/" target="_blank" rel="external">官方文档</a> 。</p>
<h4 id="Hive"><a href="#Hive" class="headerlink" title="Hive"></a>Hive</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[beeswax]</div><div class="line">  # Host where HiveServer2 is running. </div><div class="line">  # 如果Hue与Hive不在同一台服务器，需要修改指定正确的Hive服务主机.</div><div class="line">  hive_server_host=localhost</div></pre></td></tr></table></figure>
<p>有些朋友说“我从来不用Hive”，但是Hue执行交互式SQL查询是通过Hive完成的，所以还是建议乖乖的安装配置好Hive在你的集群中吧，至于怎么安装Hive不在此赘述。</p>
<h4 id="HDFS"><a href="#HDFS" class="headerlink" title="HDFS"></a>HDFS</h4><p>首先修改Hadoop配置文件，增加WebHDFS功能，支持通过web访问HDFS</p>
<ul>
<li><p>hdfs-site.xml </p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.webhdfs.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div></pre></td></tr></table></figure>
</li>
</ul>
<ul>
<li><p>core-site.xml</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.hue.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.hue.groups<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div></pre></td></tr></table></figure>
</li>
</ul>
<p>然后对接Hue配置，如果Hue与NameNode在同一台机器，保持默认即可。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">[[hdfs_clusters]]</div><div class="line">    # HA support by using HttpFs</div><div class="line">    [[[default]]]</div><div class="line">      # Enter the filesystem uri</div><div class="line">      fs_defaultfs=hdfs://node1:8020</div><div class="line"></div><div class="line">      # Use WebHdfs/HttpFs as the communication mechanism.</div><div class="line">      # Domain should be the NameNode or HttpFs host.</div><div class="line">      # Default port is 14000 for HttpFs.</div><div class="line">       webhdfs_url=http://node1:50070/webhdfs/v1</div></pre></td></tr></table></figure>
<p>另外一点需特别注意的，为了获取HDFS访问权限，务必指定Hadoop用户</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"># This should be the hadoop cluster admin</div><div class="line">   default_hdfs_superuser=your-user</div></pre></td></tr></table></figure>
<h4 id="Yarn"><a href="#Yarn" class="headerlink" title="Yarn"></a>Yarn</h4><p>监控Job，需要配置ProxyServer 和 Job History servers。<br>首先，Hadoop侧的配置修改</p>
<ul>
<li><p>mapred-site.xml (<em>Job History servers</em>)</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>$&#123;your-host&#125;:19888<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div></pre></td></tr></table></figure>
<p>启动Job History server服务</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">mr-jobhistory-daemon.sh start historyserver</div></pre></td></tr></table></figure>
</li>
</ul>
<ul>
<li><p>yarn-site.xml (<em>ProxyServer</em>)</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>$&#123;your-host&#125;:8088<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.web-proxy.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>$&#123;your-host&#125;:8888<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div></pre></td></tr></table></figure>
<p>启动ProxyServer服务    </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">yarn-daemon.sh start proxyserver</div></pre></td></tr></table></figure>
</li>
</ul>
<p>对接Hue配置</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">[hadoop]</div><div class="line">  [[yarn_clusters]]</div><div class="line">    [[[default]]]</div><div class="line">      # Enter the host on which you are running the ResourceManager</div><div class="line">      resourcemanager_host=your-host     </div><div class="line">      # Whether to submit jobs to this cluster</div><div class="line">      submit_to=True</div><div class="line">      # URL of the ResourceManager API</div><div class="line">      resourcemanager_api_url=http://your-host:8088</div><div class="line">      # URL of the ProxyServer API</div><div class="line">      proxy_api_url=http://your-host:8088</div><div class="line">      # URL of the HistoryServer API</div><div class="line">      history_server_api_url=http://your-host:19888</div></pre></td></tr></table></figure>
<p>好了，现在你可以Job Browser浏览所有运行的Job并查看日志。</p>
<h4 id="HBase"><a href="#HBase" class="headerlink" title="HBase"></a>HBase</h4><p>HBase配置相对简单，只要指定HBase服务的host即可</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[hbase]</div><div class="line"># Comma-separated list of HBase Thrift server 1 for clusters in the format of</div><div class="line"> &apos;(name|host:port)&apos;.</div><div class="line">hbase_clusters=(Cluster|your-host:9090)</div></pre></td></tr></table></figure>
<p>然后，务必记得在HMaster所在主机启动Thrift服务<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">hbase thrift start &amp;</div></pre></td></tr></table></figure></p>
<h4 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h4><p>更新中……</p>
<hr>
<h3 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h3><ol>
<li><a href="http://gethue.com/how-to-configure-hue-in-your-hadoop-cluster/" target="_blank" rel="external">how-to-configure-hue-in-your-hadoop-cluster</a></li>
<li><a href="https://github.com/cloudera/hue" target="_blank" rel="external">Hue-Github</a></li>
<li><a href="http://livy.io/quickstart.html" target="_blank" rel="external">Livy, an Open Source REST Service for Apache Spark</a> </li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Hue&lt;/p&gt;
&lt;h3 id=&quot;基础依赖&quot;&gt;&lt;a href=&quot;#基础依赖&quot; class=&quot;headerlink&quot; title=&quot;基础依赖&quot;&gt;&lt;/a&gt;基础依赖&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;JDK&lt;/li&gt;
&lt;li&gt;Maven&lt;/li&gt;
&lt;li&gt;Git（用于从github下载Hu
    
    </summary>
    
    
      <category term="大数据" scheme="https://liamlee.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Hue" scheme="https://liamlee.github.io/tags/Hue/"/>
    
  </entry>
  
  <entry>
    <title>读《嫌疑人X的献身》</title>
    <link href="https://liamlee.github.io/2016/11/08/read-xianyirenXdexianshen/"/>
    <id>https://liamlee.github.io/2016/11/08/read-xianyirenXdexianshen/</id>
    <published>2016-11-08T15:04:22.000Z</published>
    <updated>2016-11-14T06:01:40.000Z</updated>
    
    <content type="html"><![CDATA[<p>富㭴慎二：一个看似成功的汽车销售，其实是利用职务之便挥霍公款，出手阔错。追求到花岗靖子，组成了一个看似美满的家庭。东窗事发后，被公司开除，露出了游手好闲的本性，变成一个吃软饭的无赖，离婚后还阴魂不散的缠着花岗靖子。</p>
<p>花岗靖子：一个带着不算小魅力的单身女人，两度婚姻失败，原本在酒廊做陪酒女，为躲避恶棍一般的前夫，带着女儿离开原来的地方，来到朋友小代子的便当店里帮忙。她的到来，石神多了个邻居，石神有了活着的新意义。也因为她的不得已错手杀了前来纠缠的前夫，把石神带入了一条不归路。</p>
<p>石神哲哉：一个数学天才，有着强大的逻辑能力，无奈造化弄人，只能做一个高中数学老师，默默无闻的活着，甚至有过不愿意继续活着的念头。当新来的邻居靖子过来打招呼的时候，石神立即就被深深的吸引住了，似乎有了继续活下去的理由了，以至于在靖子犯下命案的时候，第一时间就出手相助，并在必要的时候献身自己。石神处理了富㭴慎二的尸体，极其隐秘的处理了；石神在第二天制造了另一起命案，杀了一个流浪汉，并直接暴露其尸体；石神制造一系列明显的又模糊的线索，并通过例外的请假制造自己无法证明的不在场证明；石神给靖子想好一系列的对付盘问的话语和方法。最后石神在汤川的识破后献身了自己。</p>
<p>草薙俊平： 一个能力不算差的刑侦警察，陷入了石神为靖子制造的完美的不在场证明里面，只能一次次的求助汤川。</p>
<p>汤川：一个物理学教授。与石神、草薙同毕业于帝都大学，与石神互相欣赏、心心相惜，可以说是石神唯一的朋友。有着聪明的头脑，经常协助警方办理疑难杂案。由于对石神有着深入的了解，通过石神的行为细节推测出了石神参与靖子杀人案的动机和切入点，并随着深入的调查被石神的献身精神深深震惊。他完全理解石神，但对老友将要深陷囵圄、才华凋谢感到惋惜、痛心。</p>
<p>花冈美里：一个两度婚姻失败女人的孩子，对成年男子心存畏惧、不信任，但应该是信任石神的。或者正是美里最后的割腕自杀行为，让靖子突破了自我保护的最后防线，走进了警局。</p>
<p>最后：石神是不是真的是一个变态，我选择相信不是。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;富㭴慎二：一个看似成功的汽车销售，其实是利用职务之便挥霍公款，出手阔错。追求到花岗靖子，组成了一个看似美满的家庭。东窗事发后，被公司开除，露出了游手好闲的本性，变成一个吃软饭的无赖，离婚后还阴魂不散的缠着花岗靖子。&lt;/p&gt;
&lt;p&gt;花岗靖子：一个带着不算小魅力的单身女人，两度
    
    </summary>
    
    
      <category term="东野圭吾" scheme="https://liamlee.github.io/tags/%E4%B8%9C%E9%87%8E%E5%9C%AD%E5%90%BE/"/>
    
      <category term="读万卷书" scheme="https://liamlee.github.io/tags/%E8%AF%BB%E4%B8%87%E5%8D%B7%E4%B9%A6/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="https://liamlee.github.io/2016/10/19/hello-world/"/>
    <id>https://liamlee.github.io/2016/10/19/hello-world/</id>
    <published>2016-10-19T05:22:14.000Z</published>
    <updated>2016-10-19T05:22:14.000Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="external">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="external">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="external">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="external">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo new <span class="string">"My New Post"</span></div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="external">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo server</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="external">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo generate</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="external">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo deploy</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="external">Deployment</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.
    
    </summary>
    
    
  </entry>
  
</feed>
