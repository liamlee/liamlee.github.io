<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Liamming</title>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://liamlee.github.io/"/>
  <updated>2016-12-29T11:46:15.000Z</updated>
  <id>https://liamlee.github.io/</id>
  
  <author>
    <name>Ming</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Spark-Notebook安装与调教</title>
    <link href="https://liamlee.github.io/2016/12/28/sparkNotebook-install/"/>
    <id>https://liamlee.github.io/2016/12/28/sparkNotebook-install/</id>
    <published>2016-12-28T03:02:46.000Z</published>
    <updated>2016-12-29T11:46:15.000Z</updated>
    
    <content type="html"><![CDATA[<p>近来，忙活了一段时间的项目被宣布暂时停下来，另外一条线的工作也因为等待数据中暂时无法开展起来，忽然间感觉所有工作都戛然而止了。那么，就领着“大数据平台规划”的任务折腾一些新的东西吧。依据“可视化数据分析”的概念摸摸索索，发现了SparkNotebook。下面就是这个工具的安装配置笔记。</p>
<h3 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h3><ul>
<li>直接去<a href="http://spark-notebook.io" target="_blank" rel="external">官网</a>下载编译好的包，作者提供了非常非常详尽的下载列表，所有压缩方式、版本支持、组件支持的组合一一列在了主页面，擦亮双眼找出你理想的版本下载即可。</li>
<li>去<a href="https://github.com/andypetrella/spark-notebook/" target="_blank" rel="external">Github主页</a>下载源码，然后自行编译。看到官网那么详细的列表，我猜想要自行编译出个性化的包一定很麻烦，所以我直接选择放弃源码编译。</li>
</ul>
<h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p>Spark Notebook的安装可以说非常简单，就是一个步骤：解~压~缩~。环境变量你想配或者不配，随你大小便咯。完成解压后，就可以运行起来了。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">cd spark-notebook-0.7.0</div><div class="line">./bin/spark-notebook</div></pre></td></tr></table></figure>
<p>在浏览器输入<code>http://your-host:9001</code>，即可访问notebook。</p>
<h3 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h3><p><em>配置永远是重头戏</em></p>
<p>配置文件列表：</p>
<ul>
<li>conf/application.conf</li>
<li>conf/application.ini</li>
<li>conf/clusters</li>
<li>conf/profiles</li>
</ul>
<p>其实尴尬了，细细盘点一下，发现没有对上述配置文件动过什么手脚。所以，只剩下一下遇坑爬坑的心得了，作为一个知识积累放在这里，如顺便能对其他查阅者有所帮助，那最好不过了。</p>
<ul>
<li><p><strong>连接已有的Spark集群来做数据分析</strong></p>
<ol>
<li><p>代码方式：在创建SparkSession时直接指定集群的master。不！奏！效！</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span></div><div class="line">      .builder()</div><div class="line">      .config(<span class="string">"spark.master"</span>,<span class="string">"spark://node1:7077"</span>)</div><div class="line">      .getOrCreate()</div></pre></td></tr></table></figure>
</li>
<li><p>配置“Notebook Metadata”方式：<em>这里演示的是standalone模式</em></p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">"customSparkConf": &#123;</div><div class="line">    "spark.app.name": "Notebook",</div><div class="line">    "spark.master": "spark://node1:7077",</div><div class="line">    "spark.executor.memory": "10G",</div><div class="line">    "spark.cores.max": 10,</div><div class="line">    "spark.executor.cores": 2</div><div class="line">  &#125;</div></pre></td></tr></table></figure>
</li>
<li><p>配置conf/clusters文件，在clusters文件中，我们可以看到作者已经针对各种集群任务提交模式写了example。我们可以修改其中的“customSparkConf”做个性化修改，配置你的集群信息，具体格式与上一点一样。</p>
</li>
</ol>
</li>
<li><p><strong>连接Hadoop读取HDFS文件</strong></p>
<ol>
<li><p>代码方式：在文件路径前添加HDFS地址前缀“hdfs://servesr/dataPath”</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> testRDD = sc.textFile(<span class="string">"hdfs://node1/data/test.txt"</span>)</div></pre></td></tr></table></figure>
</li>
<li><p>配置$HADOOP_CONF_DIR。很多人对这个可能没有感觉这里有坑。我踩坑是因为环境变量的问题，我的集群没有将HADOOP_CONF_DIR配置到系统(用户)环境变量中，只是在spark-env.sh中export了HADOOP_CONF_DIR，所以Spark集群本身没有问题。</p>
<p>爬坑过程异常烦躁，官方doc就没有提这事，估计觉着这就不是事。网上找吧，基本没有相关博客，github主页中的issue栏目也没有发现谁碰到了这个问题。无奈，翻遍上面提到的几个配置文件，也没发现这个配置项。</p>
<p>最后，只有一个地方了，启动脚本(<code>bin/spark-notebook</code>)。终于找到了线索：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">declare -r app_classpath=&quot;$&#123;CLASSPATH_OVERRIDES&#125;:$&#123;YARN_CONF_DIR&#125;:$&#123;HADOOP_CONF_DIR&#125;:$&#123;EXTRA_CLASSPATH&#125;</div></pre></td></tr></table></figure>
<p>notebook在启动的时候读取了环境变量，那就好办了，在~/.bashrc中添加HADOOP_CONF_DIR变量即可。当然YARN_CONF_DIR也是同样的道理。</p>
<p>另外提一点，这里发现<em>CLASSPATH_OVERRIDES</em>这个变量，我觉得这个变量可以配置spark-notebook自己需要的CLASSPATH，比如<em>figaro</em>(这个也是折腾我好半天)，避免常规的CLASSPATH太重。</p>
</li>
</ol>
</li>
<li><p>Metadata配置，这个暂时不表，后续使用过程中，应该主要在于包依赖配置。</p>
</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;近来，忙活了一段时间的项目被宣布暂时停下来，另外一条线的工作也因为等待数据中暂时无法开展起来，忽然间感觉所有工作都戛然而止了。那么，就领着“大数据平台规划”的任务折腾一些新的东西吧。依据“可视化数据分析”的概念摸摸索索，发现了SparkNotebook。下面就是这个工具的安
    
    </summary>
    
    
      <category term="Spark" scheme="https://liamlee.github.io/tags/Spark/"/>
    
      <category term="大数据" scheme="https://liamlee.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="安装教程" scheme="https://liamlee.github.io/tags/%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B/"/>
    
  </entry>
  
  <entry>
    <title>Hue 安装配置</title>
    <link href="https://liamlee.github.io/2016/12/26/Hue-Install/"/>
    <id>https://liamlee.github.io/2016/12/26/Hue-Install/</id>
    <published>2016-12-26T07:37:29.000Z</published>
    <updated>2016-12-29T11:51:57.000Z</updated>
    
    <content type="html"><![CDATA[<p>Hue</p>
<h3 id="基础依赖"><a href="#基础依赖" class="headerlink" title="基础依赖"></a>基础依赖</h3><ul>
<li>JDK</li>
<li>Maven</li>
<li>Git（用于从github下载Hue） </li>
<li>集群嘛，当然我认为你已经搭建好了</li>
<li>库包依赖：参考Hue的githu主页 <a href="https://github.com/cloudera/hue" target="_blank" rel="external">Hue</a> 所列的所有依赖，一一检查安装</li>
</ul>
<hr>
<h3 id="下载安装"><a href="#下载安装" class="headerlink" title="下载安装"></a>下载安装</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">git clone https://github.com/cloudera/hue.git (brach)</div><div class="line">cd hue</div><div class="line">make apps</div></pre></td></tr></table></figure>
<p>make过程时间会比较长，只要前面所提的依赖都已安装好，整个过程一般不会出现错误，耐心等待即可</p>
<h3 id="初步验证"><a href="#初步验证" class="headerlink" title="初步验证"></a>初步验证</h3><ul>
<li>在后台启动服务 <code>build/env/bin/hue runserver</code></li>
</ul>
<ul>
<li>访问<a href="http://localhost:8000。正常情况，前台出现登录界面。" target="_blank" rel="external">http://localhost:8000。正常情况，前台出现登录界面。</a></li>
</ul>
<ul>
<li>Hue初始界面没有用户注册选项，直接根据用户的的输入创建第一个超级用户，后续建立用户分配权限都依赖该用户，所以需要特别注意，务必记住自己输入了什么。</li>
</ul>
<hr>
<h3 id="配置Hue"><a href="#配置Hue" class="headerlink" title="配置Hue"></a>配置Hue</h3><p>在完成安装和初步验证后，现在进入最重要也是最复杂的步骤，配置。Hue支持Hadoop生态系统中的各种组建，比如访问HDFS文件、执行SQL查询HBase中的表、编写notebook来运行spark，前提是要做好对应的配置。</p>
<p><strong>配置文件</strong><br>从github下载的开发版，配置文件为 <strong><em>desktop/conf/pseudo-distributed.ini</em></strong> ,如果是其他发行版请参考 <a href="http://gethue.com/how-to-configure-hue-in-your-hadoop-cluster/" target="_blank" rel="external">官方文档</a> 。</p>
<h4 id="Hive"><a href="#Hive" class="headerlink" title="Hive"></a>Hive</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[beeswax]</div><div class="line">  # Host where HiveServer2 is running. </div><div class="line">  # 如果Hue与Hive不在同一台服务器，需要修改指定正确的Hive服务主机.</div><div class="line">  hive_server_host=localhost</div></pre></td></tr></table></figure>
<p>有些朋友说“我从来不用Hive”，但是Hue执行交互式SQL查询是通过Hive完成的，所以还是建议乖乖的安装配置好Hive在你的集群中吧，至于怎么安装Hive不在此赘述。</p>
<h4 id="HDFS"><a href="#HDFS" class="headerlink" title="HDFS"></a>HDFS</h4><p>首先修改Hadoop配置文件，增加WebHDFS功能，支持通过web访问HDFS</p>
<ul>
<li><p>hdfs-site.xml </p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.webhdfs.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div></pre></td></tr></table></figure>
</li>
</ul>
<ul>
<li><p>core-site.xml</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.hue.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.hue.groups<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div></pre></td></tr></table></figure>
</li>
</ul>
<p>然后对接Hue配置，如果Hue与NameNode在同一台机器，保持默认即可。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">[[hdfs_clusters]]</div><div class="line">    # HA support by using HttpFs</div><div class="line">    [[[default]]]</div><div class="line">      # Enter the filesystem uri</div><div class="line">      fs_defaultfs=hdfs://node1:8020</div><div class="line"></div><div class="line">      # Use WebHdfs/HttpFs as the communication mechanism.</div><div class="line">      # Domain should be the NameNode or HttpFs host.</div><div class="line">      # Default port is 14000 for HttpFs.</div><div class="line">       webhdfs_url=http://node1:50070/webhdfs/v1</div></pre></td></tr></table></figure>
<p>另外一点需特别注意的，为了获取HDFS访问权限，务必指定Hadoop用户</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"># This should be the hadoop cluster admin</div><div class="line">   default_hdfs_superuser=your-user</div></pre></td></tr></table></figure>
<h4 id="Yarn"><a href="#Yarn" class="headerlink" title="Yarn"></a>Yarn</h4><p>监控Job，需要配置ProxyServer 和 Job History servers。<br>首先，Hadoop侧的配置修改</p>
<ul>
<li><p>mapred-site.xml (<em>Job History servers</em>)</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>$&#123;your-host&#125;:19888<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div></pre></td></tr></table></figure>
<p>启动Job History server服务</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">mr-jobhistory-daemon.sh start historyserver</div></pre></td></tr></table></figure>
</li>
</ul>
<ul>
<li><p>yarn-site.xml (<em>ProxyServer</em>)</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>$&#123;your-host&#125;:8088<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.web-proxy.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>$&#123;your-host&#125;:8888<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div></pre></td></tr></table></figure>
<p>启动ProxyServer服务    </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">yarn-daemon.sh start proxyserver</div></pre></td></tr></table></figure>
</li>
</ul>
<p>对接Hue配置</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">[hadoop]</div><div class="line">  [[yarn_clusters]]</div><div class="line">    [[[default]]]</div><div class="line">      # Enter the host on which you are running the ResourceManager</div><div class="line">      resourcemanager_host=your-host     </div><div class="line">      # Whether to submit jobs to this cluster</div><div class="line">      submit_to=True</div><div class="line">      # URL of the ResourceManager API</div><div class="line">      resourcemanager_api_url=http://your-host:8088</div><div class="line">      # URL of the ProxyServer API</div><div class="line">      proxy_api_url=http://your-host:8088</div><div class="line">      # URL of the HistoryServer API</div><div class="line">      history_server_api_url=http://your-host:19888</div></pre></td></tr></table></figure>
<p>好了，现在你可以Job Browser浏览所有运行的Job并查看日志。</p>
<h4 id="HBase"><a href="#HBase" class="headerlink" title="HBase"></a>HBase</h4><p>HBase配置相对简单，只要指定HBase服务的host即可</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[hbase]</div><div class="line"># Comma-separated list of HBase Thrift server 1 for clusters in the format of</div><div class="line"> &apos;(name|host:port)&apos;.</div><div class="line">hbase_clusters=(Cluster|your-host:9090)</div></pre></td></tr></table></figure>
<p>然后，务必记得在HMaster所在主机启动Thrift服务<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">hbase thrift start &amp;</div></pre></td></tr></table></figure></p>
<h4 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h4><p>更新中……</p>
<hr>
<h3 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h3><ol>
<li><a href="http://gethue.com/how-to-configure-hue-in-your-hadoop-cluster/" target="_blank" rel="external">how-to-configure-hue-in-your-hadoop-cluster</a></li>
<li><a href="https://github.com/cloudera/hue" target="_blank" rel="external">Hue-Github</a></li>
<li><a href="http://livy.io/quickstart.html" target="_blank" rel="external">Livy, an Open Source REST Service for Apache Spark</a> </li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Hue&lt;/p&gt;
&lt;h3 id=&quot;基础依赖&quot;&gt;&lt;a href=&quot;#基础依赖&quot; class=&quot;headerlink&quot; title=&quot;基础依赖&quot;&gt;&lt;/a&gt;基础依赖&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;JDK&lt;/li&gt;
&lt;li&gt;Maven&lt;/li&gt;
&lt;li&gt;Git（用于从github下载Hu
    
    </summary>
    
    
      <category term="大数据" scheme="https://liamlee.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="Hue" scheme="https://liamlee.github.io/tags/Hue/"/>
    
  </entry>
  
  <entry>
    <title>读《嫌疑人X的献身》</title>
    <link href="https://liamlee.github.io/2016/11/08/read-xianyirenXdexianshen/"/>
    <id>https://liamlee.github.io/2016/11/08/read-xianyirenXdexianshen/</id>
    <published>2016-11-08T15:04:22.000Z</published>
    <updated>2016-11-14T06:01:40.000Z</updated>
    
    <content type="html"><![CDATA[<p>富㭴慎二：一个看似成功的汽车销售，其实是利用职务之便挥霍公款，出手阔错。追求到花岗靖子，组成了一个看似美满的家庭。东窗事发后，被公司开除，露出了游手好闲的本性，变成一个吃软饭的无赖，离婚后还阴魂不散的缠着花岗靖子。</p>
<p>花岗靖子：一个带着不算小魅力的单身女人，两度婚姻失败，原本在酒廊做陪酒女，为躲避恶棍一般的前夫，带着女儿离开原来的地方，来到朋友小代子的便当店里帮忙。她的到来，石神多了个邻居，石神有了活着的新意义。也因为她的不得已错手杀了前来纠缠的前夫，把石神带入了一条不归路。</p>
<p>石神哲哉：一个数学天才，有着强大的逻辑能力，无奈造化弄人，只能做一个高中数学老师，默默无闻的活着，甚至有过不愿意继续活着的念头。当新来的邻居靖子过来打招呼的时候，石神立即就被深深的吸引住了，似乎有了继续活下去的理由了，以至于在靖子犯下命案的时候，第一时间就出手相助，并在必要的时候献身自己。石神处理了富㭴慎二的尸体，极其隐秘的处理了；石神在第二天制造了另一起命案，杀了一个流浪汉，并直接暴露其尸体；石神制造一系列明显的又模糊的线索，并通过例外的请假制造自己无法证明的不在场证明；石神给靖子想好一系列的对付盘问的话语和方法。最后石神在汤川的识破后献身了自己。</p>
<p>草薙俊平： 一个能力不算差的刑侦警察，陷入了石神为靖子制造的完美的不在场证明里面，只能一次次的求助汤川。</p>
<p>汤川：一个物理学教授。与石神、草薙同毕业于帝都大学，与石神互相欣赏、心心相惜，可以说是石神唯一的朋友。有着聪明的头脑，经常协助警方办理疑难杂案。由于对石神有着深入的了解，通过石神的行为细节推测出了石神参与靖子杀人案的动机和切入点，并随着深入的调查被石神的献身精神深深震惊。他完全理解石神，但对老友将要深陷囵圄、才华凋谢感到惋惜、痛心。</p>
<p>花冈美里：一个两度婚姻失败女人的孩子，对成年男子心存畏惧、不信任，但应该是信任石神的。或者正是美里最后的割腕自杀行为，让靖子突破了自我保护的最后防线，走进了警局。</p>
<p>最后：石神是不是真的是一个变态，我选择相信不是。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;富㭴慎二：一个看似成功的汽车销售，其实是利用职务之便挥霍公款，出手阔错。追求到花岗靖子，组成了一个看似美满的家庭。东窗事发后，被公司开除，露出了游手好闲的本性，变成一个吃软饭的无赖，离婚后还阴魂不散的缠着花岗靖子。&lt;/p&gt;
&lt;p&gt;花岗靖子：一个带着不算小魅力的单身女人，两度
    
    </summary>
    
    
      <category term="东野圭吾" scheme="https://liamlee.github.io/tags/%E4%B8%9C%E9%87%8E%E5%9C%AD%E5%90%BE/"/>
    
      <category term="读万卷书" scheme="https://liamlee.github.io/tags/%E8%AF%BB%E4%B8%87%E5%8D%B7%E4%B9%A6/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="https://liamlee.github.io/2016/10/19/hello-world/"/>
    <id>https://liamlee.github.io/2016/10/19/hello-world/</id>
    <published>2016-10-19T05:22:14.000Z</published>
    <updated>2016-10-19T05:22:14.000Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="external">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="external">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="external">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="external">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo new <span class="string">"My New Post"</span></div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="external">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo server</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="external">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo generate</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="external">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo deploy</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="external">Deployment</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.
    
    </summary>
    
    
  </entry>
  
</feed>
