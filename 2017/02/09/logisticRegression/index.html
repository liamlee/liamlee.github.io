<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="Ming 的碎碎念"><title>斯坦福(吴恩达)-机器学习-第三周编程作业代码笔记 | Liamming</title><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/4.2.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.0/grids-responsive-min.css"><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.0.0/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">斯坦福(吴恩达)-机器学习-第三周编程作业代码笔记</h1><a id="logo" href="/.">Liamming</a><p class="description"></p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div id="layout" class="pure-g"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">斯坦福(吴恩达)-机器学习-第三周编程作业代码笔记</h1><div class="post-meta">Feb 9, 2017<script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> Hits</span></span></div><a data-disqus-identifier="2017/02/09/logisticRegression/" href="/2017/02/09/logisticRegression/#disqus_thread" class="disqus-comment-count"></a><div class="clear"><div id="toc" class="toc-article"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#假设函数"><span class="toc-number">1.</span> <span class="toc-text">假设函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#简单模型"><span class="toc-number">2.</span> <span class="toc-text">简单模型</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Sigmoid函数"><span class="toc-number">2.1.</span> <span class="toc-text">Sigmoid函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#CostFunction"><span class="toc-number">2.2.</span> <span class="toc-text">CostFunction</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#应用-分类"><span class="toc-number">2.3.</span> <span class="toc-text">应用-分类</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#加入正则化-让模型复杂更复杂一点"><span class="toc-number">3.</span> <span class="toc-text">加入正则化-让模型复杂更复杂一点</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#新的CostFunction"><span class="toc-number">3.1.</span> <span class="toc-text">新的CostFunction</span></a></li></ol></li></ol></div></div><div class="post-content"><h3 id="假设函数"><a href="#假设函数" class="headerlink" title="假设函数"></a>假设函数</h3><p>先把假设函数放出来，一看假设函数，明白的人就已基本明白是个怎么回事了。<br>$$<br>h_\theta(x) = g(\theta^Tx)<br>$$<br>其中(g)：<br>$$<br>g(z) = \frac{1} { 1 + e^{-z} }<br>$$<br>(g)就是所谓的sigmoid函数。那么首先解决sigmoid函数的代码。</p>
<h3 id="简单模型"><a href="#简单模型" class="headerlink" title="简单模型"></a>简单模型</h3><h4 id="Sigmoid函数"><a href="#Sigmoid函数" class="headerlink" title="Sigmoid函数"></a>Sigmoid函数</h4><p>代码很简单</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">function g = sigmoid(z)</div><div class="line">%SIGMOID Compute sigmoid function</div><div class="line">%   g = SIGMOID(z) computes the sigmoid of z.</div><div class="line"></div><div class="line">% You need to return the following variables correctly </div><div class="line">g = zeros(size(z));</div><div class="line"></div><div class="line">% ====================== YOUR CODE HERE ======================</div><div class="line">% Instructions: Compute the sigmoid of each value of z (z can be a matrix,</div><div class="line">%               vector or scalar).</div><div class="line">g = 1 ./ (1 + exp(-z));</div><div class="line">% =============================================================</div><div class="line">end</div></pre></td></tr></table></figure>
<h4 id="CostFunction"><a href="#CostFunction" class="headerlink" title="CostFunction"></a>CostFunction</h4><p>LogisticRegression问题同LinearRegression问题解决思路类似：构造costFunction，然后利用梯度下降算法求解min{costFunction}以此来求得合适的(\theta)。</p>
<p>不同之处在于，LogisticRegression的costFunction不再使用“最小二乘法”，而是利用概率模型思想，利用样本来倒推出样本出现的最大概率，即极大似然估计思想。不过，进一步深究就会发现其实“极大似然”包含着“最小二乘法”，为什么暂且不表。于是有：<br>$$<br>J(\theta) = \frac{1}{m}\sum<em>{i = 1}^m[-y^{(i)}\log(h</em>\theta(x^{(i)})) - (1 - y^{(i)})\log(1 - h_\theta(x^{(i)}))]<br>$$<br>而梯度函数：<br>$$<br>\frac{\partial J(\theta)}{\partial\theta<em>j} = \frac{1}{m}\sum</em>{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)}<br>$$<br>代码表示</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line">function [J, grad] = costFunction(theta, X, y)</div><div class="line">%COSTFUNCTION Compute cost and gradient for logistic regression</div><div class="line">%   J = COSTFUNCTION(theta, X, y) computes the cost of using theta as the</div><div class="line">%   parameter for logistic regression and the gradient of the cost</div><div class="line">%   w.r.t. to the parameters.</div><div class="line"></div><div class="line">% Initialize some useful values</div><div class="line">m = length(y); % number of training examples</div><div class="line"></div><div class="line">% You need to return the following variables correctly </div><div class="line">J = 0;</div><div class="line">grad = zeros(size(theta));</div><div class="line"></div><div class="line">% ====================== YOUR CODE HERE ======================</div><div class="line">% Instructions: Compute the cost of a particular choice of theta.</div><div class="line">%               You should set J to the cost.</div><div class="line">%               Compute the partial derivatives and set grad to the partial</div><div class="line">%               derivatives of the cost w.r.t. each parameter in theta</div><div class="line">%</div><div class="line">% Note: grad should have the same dimensions as theta</div><div class="line">%</div><div class="line">h = sigmoid(X * theta);</div><div class="line">J = (-y&apos; * log(h) - (1 - y)&apos; * log(1 - h)) / m;</div><div class="line">grad = ((h - y)&apos; * X) / m;</div><div class="line">% =============================================================</div><div class="line">end</div></pre></td></tr></table></figure>
<h4 id="应用-分类"><a href="#应用-分类" class="headerlink" title="应用-分类"></a>应用-分类</h4><p>利用梯度下降算法求得合适的(\theta)后，也就得到的模型，就可以应用于test数据的分类了。当然，先得把分类方法实现：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">function p = predict(theta, X)</div><div class="line">%PREDICT Predict whether the label is 0 or 1 using learned logistic </div><div class="line">%regression parameters theta</div><div class="line">%   p = PREDICT(theta, X) computes the predictions for X using a </div><div class="line">%   threshold at 0.5 (i.e., if sigmoid(theta&apos;*x) &gt;= 0.5, predict 1)</div><div class="line"></div><div class="line">m = size(X, 1); % Number of training examples</div><div class="line"></div><div class="line">% You need to return the following variables correctly</div><div class="line">p = zeros(m, 1);</div><div class="line"></div><div class="line">% ====================== YOUR CODE HERE ======================</div><div class="line">% Instructions: Complete the following code to make predictions using</div><div class="line">%               your learned logistic regression parameters. </div><div class="line">%               You should set p to a vector of 0&apos;s and 1&apos;s</div><div class="line">%</div><div class="line"></div><div class="line">p = round(sigmoid(X * theta));</div><div class="line"></div><div class="line">% =========================================================================</div><div class="line"></div><div class="line">end</div></pre></td></tr></table></figure>
<p>到此，一个简单的LogisticRegression模型就实现了。</p>
<h3 id="加入正则化-让模型复杂更复杂一点"><a href="#加入正则化-让模型复杂更复杂一点" class="headerlink" title="加入正则化-让模型复杂更复杂一点"></a>加入正则化-让模型复杂更复杂一点</h3><p>当数据分布比较复杂时，比如无法用简单的一条直线将数据分类开来。本练习中介绍一种构造更多的特征的方法，通过对原特征数据进行映射得到新的特征来训练模型，更多维的特征数据可以让模型训练得到更复杂的决策分界线，以此来达到将数据最优分类的目的。</p>
<p>随着特征维度变高变复杂，我们将训练得到更复杂的模型，而越复杂的模型会越可能出现对数据过拟合的现象，引入正则化正是为了克服过拟合的问题。</p>
<p>额，这一段是我自己编的，准不准确看您的功力来分辨了。</p>
<p>本练习重点关注正则化，至于特征构造课程已给出实现代码供于参考。</p>
<h4 id="新的CostFunction"><a href="#新的CostFunction" class="headerlink" title="新的CostFunction"></a>新的CostFunction</h4><p>$$<br>J(\theta) = \frac{1}{m}\sum<em>{i = 1}^m[-y^{(i)}\log(h</em>\theta(x^{(i)})) - (1 - y^{(i)})\log(1 - h<em>\theta(x^{(i)}))] + \frac{\lambda}{2m}\sum</em>{j=1}^n\theta_j^2<br>$$</p>
<p>对应新的梯度函数<br>$$<br>\frac{\partial J(\theta)}{\partial \theta<em>0} = \frac{1}{m}\sum</em>{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)}  \ \ \ \ \ \ \ \ \textrm{for $j = $0}<br>$$</p>
<p>$$<br>\frac{\partial J(\theta)}{\partial \theta<em>j} = (\frac{1}{m}\sum</em>{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)}) + \frac{\lambda}{m}\theta_j  \ \ \ \ \ \ \ \ \textrm{for $j &gt;= $1}<br>$$</p>
<p>这里需要特别注意的是，(\theta_0)是不需要做正则化处理的，所以代码实现上也得注意了：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line">function [J, grad] = costFunctionReg(theta, X, y, lambda)</div><div class="line">%COSTFUNCTIONREG Compute cost and gradient for logistic regression with regularization</div><div class="line">%   J = COSTFUNCTIONREG(theta, X, y, lambda) computes the cost of using</div><div class="line">%   theta as the parameter for regularized logistic regression and the</div><div class="line">%   gradient of the cost w.r.t. to the parameters. </div><div class="line"></div><div class="line">% Initialize some useful values</div><div class="line">m = length(y); % number of training examples</div><div class="line"></div><div class="line">% You need to return the following variables correctly </div><div class="line">J = 0;</div><div class="line">grad = zeros(size(theta));</div><div class="line"></div><div class="line">% ====================== YOUR CODE HERE ======================</div><div class="line">% Instructions: Compute the cost of a particular choice of theta.</div><div class="line">%               You should set J to the cost.</div><div class="line">%               Compute the partial derivatives and set grad to the partial</div><div class="line">%               derivatives of the cost w.r.t. each parameter in theta</div><div class="line"></div><div class="line">h = sigmoid(X * theta);</div><div class="line">J = (-y' * log(h) - (1 - y)' * log(1 - h)) / m + lambda / 2 / m * (theta' * theta - (theta(1,1))^2);</div><div class="line"></div><div class="line">%Note that you should not regularize the parameter θ0</div><div class="line">grad(1,1) = ((h - y)' * X(:,1)) / m;</div><div class="line"></div><div class="line">n = size(theta);</div><div class="line">% 不是很熟悉matlab，所以用个循环来实现，自我感觉跟清晰易懂一些。</div><div class="line">for i = 2:n</div><div class="line">  grad(i) = ((h - y)' * X(:,i)) / m + lambda / m * theta(i);</div><div class="line">end</div><div class="line">% =============================================================</div><div class="line"></div><div class="line">end</div></pre></td></tr></table></figure>
<p>好了，不管代码美不美，又是一次100分。</p>
<blockquote>
<p>注：本章练习中，利用了Octave/MATLAB内置的fminunc函数来实现costFunction最小化求解，所以没有再做梯度下降算法的实现。</p>
</blockquote>
<p>​<br>​<br>​    </p>
</div><script type="text/javascript" src="/js/share.js?v=0.0.0" async></script><a data-url="https://liamlee.github.io/2017/02/09/logisticRegression/" data-id="ciyy497bo0002mvs6mfabujht" class="article-share-link">分享到</a><div class="tags"><a href="/tags/机器学习/">机器学习</a><a href="/tags/LogisticRegression/">LogisticRegression</a></div><div class="post-nav"><a href="/2017/01/20/Chapter1_UsingNeuralNetsToRecognizeHandwrittenDigits/" class="next">第一章 使用神经网络识别手写体数字</a></div><div id="disqus_thread"><script>var disqus_shortname = 'liamming1990';
var disqus_identifier = '2017/02/09/logisticRegression/';
var disqus_title = '斯坦福(吴恩达)-机器学习-第三周编程作业代码笔记';
var disqus_url = 'https://liamlee.github.io/2017/02/09/logisticRegression/';
(function() {
  var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
  dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();</script><script id="dsq-count-scr" src="//liamming1990.disqus.com/count.js" async></script></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank" class="search-form"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="https://liamlee.github.io"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/总结/" style="font-size: 15px;">总结</a> <a href="/tags/神经网络/" style="font-size: 15px;">神经网络</a> <a href="/tags/图像识别/" style="font-size: 15px;">图像识别</a> <a href="/tags/机器学习/" style="font-size: 15px;">机器学习</a> <a href="/tags/LogisticRegression/" style="font-size: 15px;">LogisticRegression</a> <a href="/tags/线性回归/" style="font-size: 15px;">线性回归</a> <a href="/tags/LinearRegression/" style="font-size: 15px;">LinearRegression</a> <a href="/tags/手写体识别/" style="font-size: 15px;">手写体识别</a> <a href="/tags/东野圭吾/" style="font-size: 15px;">东野圭吾</a> <a href="/tags/读万卷书/" style="font-size: 15px;">读万卷书</a> <a href="/tags/大数据/" style="font-size: 15px;">大数据</a> <a href="/tags/Hue/" style="font-size: 15px;">Hue</a> <a href="/tags/Spark/" style="font-size: 15px;">Spark</a> <a href="/tags/安装教程/" style="font-size: 15px;">安装教程</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最新文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2017/02/09/logisticRegression/">斯坦福(吴恩达)-机器学习-第三周编程作业代码笔记</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/01/20/Chapter1_UsingNeuralNetsToRecognizeHandwrittenDigits/">第一章 使用神经网络识别手写体数字</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/01/16/linearRegression/">斯坦福(吴恩达)-机器学习-第二周编程作业代码笔记</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/12/29/look-back-2016/">回首2016</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/12/28/sparkNotebook-install/">Spark-Notebook安装与调教</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/12/26/Hue-Install/">Hue 安装配置</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/11/08/read-xianyirenXdexianshen/">读《嫌疑人X的献身》</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/10/19/hello-world/">Hello World</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-comment-o"> 最近评论</i></div><script type="text/javascript" src="//liamming1990.disqus.com/recent_comments_widget.js?num_items=5&amp;hide_avatars=1&amp;avatar_size=32&amp;excerpt_length=20&amp;hide_mods=1"></script></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">© <a href="/." rel="nofollow">Liamming.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a id="rocket" href="#top" class="show"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.pack.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="/css/jquery.fancybox.css?v=0.0.0"><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create','UA-86048320-1','auto');ga('send','pageview');
</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>