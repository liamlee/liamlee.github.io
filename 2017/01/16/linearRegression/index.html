<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="Ming 的碎碎念"><title>斯坦福(吴恩达)-机器学习-第二周编程作业代码笔记 | Liamming</title><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/4.2.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.0/grids-responsive-min.css"><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.0.0/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">斯坦福(吴恩达)-机器学习-第二周编程作业代码笔记</h1><a id="logo" href="/.">Liamming</a><p class="description"></p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div id="layout" class="pure-g"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">斯坦福(吴恩达)-机器学习-第二周编程作业代码笔记</h1><div class="post-meta">Jan 16, 2017<script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> Hits</span></span></div><a data-disqus-identifier="2017/01/16/linearRegression/" href="/2017/01/16/linearRegression/#disqus_thread" class="disqus-comment-count"></a><div class="clear"><div id="toc" class="toc-article"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-4"><a class="toc-link" href="#假设函数"><span class="toc-number">1.</span> <span class="toc-text">假设函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#computeCost"><span class="toc-number">2.</span> <span class="toc-text">computeCost</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#computeCost-多维"><span class="toc-number">3.</span> <span class="toc-text">computeCost-多维</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#梯度下降"><span class="toc-number">4.</span> <span class="toc-text">梯度下降</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#梯度下降-多维"><span class="toc-number">5.</span> <span class="toc-text">梯度下降-多维</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#特征正则化"><span class="toc-number">6.</span> <span class="toc-text">特征正则化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Normal-Equation-正规方程"><span class="toc-number">7.</span> <span class="toc-text">Normal Equation(正规方程)</span></a></li></ol></div></div><div class="post-content"><h4 id="假设函数"><a href="#假设函数" class="headerlink" title="假设函数"></a>假设函数</h4><p>以下编程逻辑基于此处定义的假设函数<br>$$<br>h_\theta(x) = \theta^T\vec x<br>$$</p>
<p>$$<br>h_\theta(x) = \theta_0 + \theta_1x<br>$$</p>
<h4 id="computeCost"><a href="#computeCost" class="headerlink" title="computeCost"></a>computeCost</h4><p>回归问题中，通常选用Square error function，也就是我们所说的“最小二乘法”</p>
<p>$$<br>J(\theta) = \frac{1} {2m}\sum<em>{i=1}^m(h</em>\theta(x^{(i)}) - y^{(i)})^2<br>$$</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">function J = computeCost(X, y, theta)</div><div class="line">%COMPUTECOST Compute cost for linear regression</div><div class="line">%   J = COMPUTECOST(X, y, theta) computes the cost of using theta as the</div><div class="line">%   parameter for linear regression to fit the data points in X and y</div><div class="line"></div><div class="line">% Initialize some useful values</div><div class="line">m = length(y); % number of training examples</div><div class="line"></div><div class="line">% You need to return the following variables correctly </div><div class="line">J = 0;</div><div class="line"></div><div class="line">% ====================== YOUR CODE HERE ======================</div><div class="line">% Instructions: Compute the cost of a particular choice of theta</div><div class="line">%               You should set J to the cost.</div><div class="line">% 注释掉烂代码 2017-02-08(由于不熟悉matlab，虽然实现了，但是代码太难看)</div><div class="line">%h = theta&apos; .* X ;</div><div class="line">%J = sum((h(:,1) + h(:,2) - y) .^ 2) / m / 2 ;</div><div class="line">% 注释掉烂代码 2017-02-08(由于不熟悉matlab，虽然实现了，但是代码太难看)</div><div class="line">J = sum((X * theta - y) .^ 2) / m / 2;</div><div class="line"></div><div class="line">% =========================================================================</div><div class="line"></div><div class="line">end</div></pre></td></tr></table></figure>
<h4 id="computeCost-多维"><a href="#computeCost-多维" class="headerlink" title="computeCost-多维"></a>computeCost-多维</h4><p>多维情况下，为了更直观的指导编程，我们可以对函数表达式做一下改写<br>$$<br>J(\theta) = \frac{1}{2m}(X\theta - \vec y)^T(X\theta - \vec y)<br>$$</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line">function J = computeCostMulti(X, y, theta)</div><div class="line">%COMPUTECOSTMULTI Compute cost for linear regression with multiple variables</div><div class="line">%   J = COMPUTECOSTMULTI(X, y, theta) computes the cost of using theta as the</div><div class="line">%   parameter for linear regression to fit the data points in X and y</div><div class="line"></div><div class="line">% Initialize some useful values</div><div class="line">m = length(y); % number of training examples</div><div class="line"></div><div class="line">% You need to return the following variables correctly </div><div class="line">J = 0;</div><div class="line"></div><div class="line">% ====================== YOUR CODE HERE ======================</div><div class="line">% Instructions: Compute the cost of a particular choice of theta</div><div class="line">%               You should set J to the cost.</div><div class="line">% J = 1/(2*m) * (theta * X&apos; - y)&apos;(theta * X&apos; - y)</div><div class="line">% 注释掉烂代码 2017-02-08(由于不熟悉matlab，虽然实现了，但是代码太难看)</div><div class="line">%h = theta&apos; .* X ;</div><div class="line">%s = size(h,2);</div><div class="line">%j = h(:,1);</div><div class="line">%for i = 2:s</div><div class="line">%  j = j + h(:,i);</div><div class="line">%end</div><div class="line">%j = j - y;</div><div class="line">%J = sum(sum(j&apos; * j)) / m / 2;</div><div class="line">% 注释掉烂代码 2017-02-08(由于不熟悉matlab，虽然实现了，但是代码太难看)</div><div class="line">J = sum((X * theta - y)&apos; * (X * theta - y)) / m / 2; </div><div class="line"></div><div class="line">% =========================================================================</div><div class="line">end</div></pre></td></tr></table></figure>
<h4 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h4><p>批量梯度下降迭代过程中，每次迭代同时更新所有<em>theta</em>，并且使用所有训练样本</p>
<p>$$<br>\theta_j := \theta<em>j - \alpha\frac1m\sum</em>{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)}<br>$$</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line">function [theta, J_history] = gradientDescent(X, y, theta, alpha, num_iters)</div><div class="line">%GRADIENTDESCENT Performs gradient descent to learn theta</div><div class="line">%   theta = GRADIENTDESCENT(X, y, theta, alpha, num_iters) updates theta by </div><div class="line">%   taking num_iters gradient steps with learning rate alpha</div><div class="line"></div><div class="line">% Initialize some useful values</div><div class="line">m = length(y); % number of training examples</div><div class="line">J_history = zeros(num_iters, 1);</div><div class="line"></div><div class="line">for iter = 1:num_iters</div><div class="line">    % ====================== YOUR CODE HERE ======================</div><div class="line">    % Instructions: Perform a single gradient step on the parameter vector</div><div class="line">    %               theta. </div><div class="line">    %</div><div class="line">    % Hint: While debugging, it can be useful to print out the values</div><div class="line">    %       of the cost function (computeCost) and gradient here.</div><div class="line">    %</div><div class="line">    % 注释掉烂代码 2017-02-08(由于不熟悉matlab，虽然实现了，但是代码太难看)</div><div class="line">    %h = theta&apos; .* X ;</div><div class="line">    </div><div class="line">    %theta(1) = theta(1) - sum((h(:,1) + h(:,2) - y)&apos; * X(:,1)) * alpha / m;</div><div class="line">    %theta(2) = theta(2) - sum((h(:,1) + h(:,2) - y)&apos; * X(:,2)) * alpha / m;</div><div class="line">    % 注释掉烂代码 2017-02-08(由于不熟悉matlab，虽然实现了，但是代码太难看)</div><div class="line">    </div><div class="line">    h = X * theta ;</div><div class="line">    theta(1) = theta(1) - sum((h - y)&apos; * X(:,1)) * alpha / m;</div><div class="line">    theta(2) = theta(2) - sum((h - y)&apos; * X(:,2)) * alpha / m;</div><div class="line"></div><div class="line">    % ============================================================</div><div class="line"></div><div class="line">    % Save the cost J in every iteration    </div><div class="line">    J_history(iter) = computeCost(X, y, theta);</div><div class="line">end</div></pre></td></tr></table></figure>
<h4 id="梯度下降-多维"><a href="#梯度下降-多维" class="headerlink" title="梯度下降-多维"></a>梯度下降-多维</h4><p>多维与前面特殊情况一样，需要注意的在编写程序时确保能支持任意维度的求解，而不是前面固定只求<em>theta1</em>、<em>theta2</em></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div></pre></td><td class="code"><pre><div class="line">function [theta, J_history] = gradientDescentMulti(X, y, theta, alpha, num_iters)</div><div class="line">%GRADIENTDESCENTMULTI Performs gradient descent to learn theta</div><div class="line">%   theta = GRADIENTDESCENTMULTI(x, y, theta, alpha, num_iters) updates theta by</div><div class="line">%   taking num_iters gradient steps with learning rate alpha</div><div class="line"></div><div class="line">% Initialize some useful values</div><div class="line">m = length(y); % number of training examples</div><div class="line">J_history = zeros(num_iters, 1);</div><div class="line">for iter = 1:num_iters</div><div class="line"></div><div class="line">    % ====================== YOUR CODE HERE ======================</div><div class="line">    % Instructions: Perform a single gradient step on the parameter vector</div><div class="line">    %               theta. </div><div class="line">    %</div><div class="line">    % Hint: While debugging, it can be useful to print out the values</div><div class="line">    %       of the cost function (computeCostMulti) and gradient here.</div><div class="line">    %</div><div class="line">    % 注释掉烂代码 2017-02-08(由于不熟悉matlab，虽然实现了，但是代码太难看)</div><div class="line">    %h = theta&apos; .* X ;</div><div class="line">    %s = size(h,2);</div><div class="line">    %j = h(:,1);</div><div class="line">    %for i = 2:s</div><div class="line">    %  j = j + h(:,i);</div><div class="line">    %end</div><div class="line">    %j = j - y;</div><div class="line">    %s = size(theta,1);</div><div class="line">    %for i = 1:s</div><div class="line">    %  theta(i) = theta(i) - sum(j&apos; * X(:,i)) * alpha / m;</div><div class="line">    %end</div><div class="line">    % 注释掉烂代码 2017-02-08(由于不熟悉matlab，虽然实现了，但是代码太难看)</div><div class="line">    </div><div class="line">    h = X * theta;</div><div class="line">    s = size(theta,1);</div><div class="line">    for i = 1:s</div><div class="line">      theta(i) = theta(i) - sum((h - y)&apos; * X(:,i)) * alpha / m;</div><div class="line">    end</div><div class="line">    % ============================================================</div><div class="line"></div><div class="line">    % Save the cost J in every iteration    </div><div class="line">    J_history(iter) = computeCostMulti(X, y, theta);</div><div class="line">end</div></pre></td></tr></table></figure>
<h4 id="特征正则化"><a href="#特征正则化" class="headerlink" title="特征正则化"></a>特征正则化</h4><p>采用均值和标准差对特征数据做正则化</p>
<ul>
<li>求得每个维度的均值、标准差</li>
<li>每个维度减去该维度的均值，然后除以标准差</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line">function [X_norm, mu, sigma] = featureNormalize(X)</div><div class="line">%FEATURENORMALIZE Normalizes the features in X </div><div class="line">%   FEATURENORMALIZE(X) returns a normalized version of X where</div><div class="line">%   the mean value of each feature is 0 and the standard deviation</div><div class="line">%   is 1. This is often a good preprocessing step to do when</div><div class="line">%   working with learning algorithms.</div><div class="line"></div><div class="line">% You need to set these values correctly</div><div class="line">X_norm = X;</div><div class="line">mu = zeros(1, size(X, 2));</div><div class="line">sigma = zeros(1, size(X, 2));</div><div class="line"></div><div class="line">% ====================== YOUR CODE HERE ======================</div><div class="line">% Instructions: First, for each feature dimension, compute the mean</div><div class="line">%               of the feature and subtract it from the dataset,</div><div class="line">%               storing the mean value in mu. Next, compute the </div><div class="line">%               standard deviation of each feature and divide</div><div class="line">%               each feature by it&apos;s standard deviation, storing</div><div class="line">%               the standard deviation in sigma. </div><div class="line">%</div><div class="line">%               Note that X is a matrix where each column is a </div><div class="line">%               feature and each row is an example. You need </div><div class="line">%               to perform the normalization separately for </div><div class="line">%               each feature. </div><div class="line">%</div><div class="line">% Hint: You might find the &apos;mean&apos; and &apos;std&apos; functions useful.</div><div class="line">%       </div><div class="line">mu = mean(X);</div><div class="line">sigma = std(X);</div><div class="line"></div><div class="line">s = size(X,2);</div><div class="line">for iter = 1:s</div><div class="line">  X_norm(:,iter) = (X_norm(:,iter) - mu(iter)) ./ sigma(iter);</div><div class="line">end</div><div class="line">% ============================================================</div><div class="line">end</div></pre></td></tr></table></figure>
<h4 id="Normal-Equation-正规方程"><a href="#Normal-Equation-正规方程" class="headerlink" title="Normal Equation(正规方程)"></a>Normal Equation(正规方程)</h4><p>根据假设函数，我们可以通过方程转换的到Normal Equation表达式直接计算得到<em>theta</em><br>$$<br>\theta = (X^TX)^{-1}X^ T\vec y<br>$$</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">function [theta] = normalEqn(X, y)</div><div class="line">%NORMALEQN Computes the closed-form solution to linear regression </div><div class="line">%   NORMALEQN(X,y) computes the closed-form solution to linear </div><div class="line">%   regression using the normal equations.</div><div class="line"></div><div class="line">theta = zeros(size(X, 2), 1);</div><div class="line"></div><div class="line">% ====================== YOUR CODE HERE ======================</div><div class="line">% Instructions: Complete the code to compute the closed form solution</div><div class="line">%               to linear regression and put the result in theta.</div><div class="line">%</div><div class="line"></div><div class="line">% ---------------------- Sample Solution ----------------------</div><div class="line">theta = (X&apos; * X)^(-1) * X&apos; * y;</div><div class="line">% -------------------------------------------------------------</div><div class="line">% ============================================================</div><div class="line">end</div></pre></td></tr></table></figure>
</div><script type="text/javascript" src="/js/share.js?v=0.0.0" async></script><a data-url="https://liamlee.github.io/2017/01/16/linearRegression/" data-id="ciywu56te0005pos6mvkjkdjr" class="article-share-link">分享到</a><div class="tags"><a href="/tags/机器学习/">机器学习</a><a href="/tags/线性回归/">线性回归</a><a href="/tags/LinearRegression/">LinearRegression</a></div><div class="post-nav"><a href="/2017/01/20/Chapter1_UsingNeuralNetsToRecognizeHandwrittenDigits/" class="pre">第一章 使用神经网络识别手写体数字</a><a href="/2016/12/29/look-back-2016/" class="next">回首2016</a></div><div id="disqus_thread"><script>var disqus_shortname = 'liamming1990';
var disqus_identifier = '2017/01/16/linearRegression/';
var disqus_title = '斯坦福(吴恩达)-机器学习-第二周编程作业代码笔记';
var disqus_url = 'https://liamlee.github.io/2017/01/16/linearRegression/';
(function() {
  var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
  dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();</script><script id="dsq-count-scr" src="//liamming1990.disqus.com/count.js" async></script></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank" class="search-form"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="https://liamlee.github.io"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/机器学习/" style="font-size: 15px;">机器学习</a> <a href="/tags/总结/" style="font-size: 15px;">总结</a> <a href="/tags/读万卷书/" style="font-size: 15px;">读万卷书</a> <a href="/tags/Spark/" style="font-size: 15px;">Spark</a> <a href="/tags/大数据/" style="font-size: 15px;">大数据</a> <a href="/tags/安装教程/" style="font-size: 15px;">安装教程</a> <a href="/tags/东野圭吾/" style="font-size: 15px;">东野圭吾</a> <a href="/tags/线性回归/" style="font-size: 15px;">线性回归</a> <a href="/tags/LinearRegression/" style="font-size: 15px;">LinearRegression</a> <a href="/tags/神经网络/" style="font-size: 15px;">神经网络</a> <a href="/tags/手写体识别/" style="font-size: 15px;">手写体识别</a> <a href="/tags/图像识别/" style="font-size: 15px;">图像识别</a> <a href="/tags/Hue/" style="font-size: 15px;">Hue</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最新文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2017/01/20/Chapter1_UsingNeuralNetsToRecognizeHandwrittenDigits/">第一章 使用神经网络识别手写体数字</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/01/16/linearRegression/">斯坦福(吴恩达)-机器学习-第二周编程作业代码笔记</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/12/29/look-back-2016/">回首2016</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/12/28/sparkNotebook-install/">Spark-Notebook安装与调教</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/12/26/Hue-Install/">Hue 安装配置</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/11/08/read-xianyirenXdexianshen/">读《嫌疑人X的献身》</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/10/19/hello-world/">Hello World</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-comment-o"> 最近评论</i></div><script type="text/javascript" src="//liamming1990.disqus.com/recent_comments_widget.js?num_items=5&amp;hide_avatars=1&amp;avatar_size=32&amp;excerpt_length=20&amp;hide_mods=1"></script></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">© <a href="/." rel="nofollow">Liamming.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a id="rocket" href="#top" class="show"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.pack.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="/css/jquery.fancybox.css?v=0.0.0"><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create','UA-86048320-1','auto');ga('send','pageview');
</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>