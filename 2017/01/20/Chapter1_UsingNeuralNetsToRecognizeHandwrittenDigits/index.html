<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="Ming 的碎碎念"><title>第一章 使用神经网络识别手写体数字 | Liamming</title><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/4.2.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.0/grids-responsive-min.css"><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.0.0/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">第一章 使用神经网络识别手写体数字</h1><a id="logo" href="/.">Liamming</a><p class="description"></p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div id="layout" class="pure-g"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">第一章 使用神经网络识别手写体数字</h1><div class="post-meta">Jan 20, 2017<script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> Hits</span></span></div><a data-disqus-identifier="2017/01/20/Chapter1_UsingNeuralNetsToRecognizeHandwrittenDigits/" href="/2017/01/20/Chapter1_UsingNeuralNetsToRecognizeHandwrittenDigits/#disqus_thread" class="disqus-comment-count"></a><div class="clear"><div id="toc" class="toc-article"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#引子"><span class="toc-number">1.</span> <span class="toc-text">引子</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#感知器"><span class="toc-number">2.</span> <span class="toc-text">感知器</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Sigmoid神经元"><span class="toc-number">3.</span> <span class="toc-text">Sigmoid神经元</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#练习"><span class="toc-number">3.1.</span> <span class="toc-text">练习</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#神经网络的结构"><span class="toc-number">4.</span> <span class="toc-text">神经网络的结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#一个简易的手写体数字识别神经网络"><span class="toc-number">5.</span> <span class="toc-text">一个简易的手写体数字识别神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#练习-1"><span class="toc-number">5.1.</span> <span class="toc-text">练习</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#用梯度下降进行学习"><span class="toc-number">6.</span> <span class="toc-text">用梯度下降进行学习</span></a></li></ol></div></div><div class="post-content"><blockquote>
<p>说明：本系列文章不是原创，</p>
<p>而是在学习<a href="http://neuralnetworksanddeeplearning.com/index.html" target="_blank" rel="external">Neural Networks and Deep Learning</a> 的时候顺便做的翻译。</p>
<p>正值春节假期，没有工作上的事劳心劳神。</p>
<p>拜年、聚会的觥筹交错之余，正好为自己充充电。</p>
<p>翻译这些文章也是心血来潮的决定，只是想为什么不通过边学边译来加深学习效果呢？</p>
<p>给自己一个挑战也是极好的。</p>
</blockquote>
<h2 id="引子"><a href="#引子" class="headerlink" title="引子"></a>引子</h2><p>人类的视觉系统可谓是一个世界奇迹。观察下面的手写体数字序列：</p>
<p><img src="http://neuralnetworksanddeeplearning.com/images/digits.png" alt=""></p>
<p>大多数人可以毫不费力的识别出这串数字是“504192”。这种“毫不费力”是表面性的。人类大脑两个半球都有一个基本的视觉皮层，我们也称之为\(V_1\)，它们包含了14亿个神经元，这些神经元构成了上百亿的连接。但是人类的视觉不仅仅只涉及\(V_1\)，而是由整个视觉皮质完成——\(V_2，V_3，V_4，V_5\)——每个层级依次完成更复杂的图像处理。我们就像携带了一个超级计算机在我们的大脑，经过数百万年的进化，可以非常好的理解这个视觉性的世界。识别手写体数字其实并不容易。相反的，我们人类对我们眼中的世界表现出惊人的认知能力。但这些都是在无意识中做到的，所以我们很少意识到我们的视觉系统在这个过程中完成了多么复杂的工作。</p>
<p>如果你试图编写一段程序让计算机来识别上面的数字，你就明显体会到其中的困难。看上去我们自己很容易做到的事情在计算机那里瞬间变得异常的困难。人类很简单的识别形状的直觉——一个9是上面一个圈，右下方一条竖直线——如果用算法来表示则非常困难。当你想精确的定义这种规则时，你会很快迷失在各种异常情况、特殊情况中，很难理出头绪来。</p>
<p>神经网络通过另一种途径来解决这个问题。它的核心思想是，收集大量的手写体样本，通常称之为训练样本，</p>
<p><img src="http://neuralnetworksanddeeplearning.com/images/mnist_100_digits.png" alt=""></p>
<p>然后开发一个能从这些样本进行学习的系统。换句话说，神经网络利用训练样本自动的推断识别手写体数字的规则。此外，神经网络的识别准确率可以随着样本的增多而提高。我在上面的样例图中只展示了100个训练数字样本，或许我们可以使用数以千计或者数百万甚至上亿的训练样本来建立更好的书写体识别系统。</p>
<p>本章我们将编程实现一个能够通过学习去识别手写体数字的神经网络。这个程序将只有74行代码，不适用任何专门的神经网络库。在没有人工干预的情况下，这个简单的程序的准确率将不低于96%。此外，我们会在接下来的章节中探索如何将准确率提升到99%以上。事实上，最好的商业神经网络已经有非常好的表现，银行用它们来做流程检查、邮局利用它们来做地址识别。</p>
<p>我们聚焦于手写体识别，是因为这是一个很典型的问题，对我们学习神经网络来说算得上一个绝佳的原型。原因有两点：首先，具有挑战性。手写体数字的识别不是小菜一碟的问题，同时也没有那么的难，它不要求非常复杂的方法或者大规模的计算能力。其次，我们可以通过它来探索更先进的技术，比如深度学习。所以整本书中，我们会反复的聚焦到手写体识别问题上。在本书后续章节中，我们将讨论如何将这种思想应用到其他领域，比如计算机视觉、自然语言处理等等。</p>
<p>当然，如果本章只是要编写一个能够识别手写体数字的程序，那么本章节将会很简短。但是，我们会在这个过程中探索很多关于神经网络的关键思想，包括两种重要的人工神经元(感知器和sigmoid神经元)，以及标准的神经网络算法，即随机梯度下降。从始至终，我聚焦于揭示事物(神经网络)的本质，帮助你建立你的神经网络思维。这就需要一个漫长的讨论过程，而不是仅仅简单的罗列基本原理和概念。为了让书本前的你有更深的认识，非常值得多费一些笔墨。当达到本书的最后时，我们将知道什么是深度学习，以及深度学习为什么如此的重要。</p>
<h2 id="感知器"><a href="#感知器" class="headerlink" title="感知器"></a>感知器</h2><p>什么是神经网络？首先，让我来阐释一种人工神经元：感知器。感知器最早由科学家<a href="http://en.wikipedia.org/wiki/Frank_Rosenblatt" target="_blank" rel="external">Frank Rosenblatt</a> 受到<a href="http://en.wikipedia.org/wiki/Warren_McCulloch" target="_blank" rel="external">Warren McCulloch</a> 和 <a href="http://en.wikipedia.org/wiki/Walter_Pitts" target="_blank" rel="external">Walter Pitts</a> 的早期<a href="http://scholar.google.ca/scholar?cluster=4035975255085082870" target="_blank" rel="external">工作</a> 的启发，在1950~1960间开发出来。在今天，另一种人工神经元“sigmoid”神经元更广泛应用于现代的神经网络中，也是本书重点要介绍的。虽然我们不用感知器这种神经元，但是为了更好的理解“sigmoid”神经元为什么如此设计，我们还是先花点时间了解一下感知器。</p>
<p>那么，感知器是如何工作的呢？一个感知器接收几个二进制输入，\(x_1, x_2, \ldots\)，然后输出一个的二进制结果：</p>
<p><img src="http://neuralnetworksanddeeplearning.com/images/tikz0.png" alt=""></p>
<p>在上面的图例中，感知器有三个输入, \(x_1,x_2,x_3\)。通常它可以有更少或者更多的输入。Rosenblatt提出一个简单的计算输出的规则。他引入了权重，\(w_1,w_2,\ldots\)，用实数来表示各个输入对输出的重要程度。神经元的输出，0或者1，由输入的加权和值\(\sum_j w_j x_j\)是否大于某个阀值来决定。跟权重一样，阀值同样也是神经元的一个实数参数。用精确的数学表达式表示：<br>$$<br>\begin{eqnarray}<br>  \mbox{output} &amp; = &amp; \left{ \begin{array}{ll}<br>      0 &amp; \mbox{if } \sum_j w_j x_j \leq \mbox{ threshold} \<br>      1 &amp; \mbox{if } \sum_j w_j x_j &gt; \mbox{ threshold}<br>      \end{array} \right.<br>\tag{1}\end{eqnarray}<br>$$<br>这就是感知器的工作方式！</p>
<p>以上是基本的数学模型。你可以这么来看待一个感知器：通过对证据进行加权然后做出决策的一种设备。让我来举个不是很实际但是很容易理解的例子，紧接着我们会看到一些更实际的例子。假设周末临近，你得到消息你所在的城市将要举行一场奶酪节。你非常喜欢奶酪，正愁要不要去参加。你可能要权衡一下以下因素然后决定去还是不去：</p>
<ol>
<li>当天天气是不是够好？</li>
<li>你的男(女)朋友会跟你一起去吗？</li>
<li>现场离公交站远不远(你没有车)？</li>
</ol>
<p>我们用相应的二进制变量\(x_1,x_2,x_3\)表示这三个因素。例如，\(x_1 = 1\)表示好天气，\(x_1 = 0\)表示坏天气；类似的，\(x_2 = 1\)表示你的男/女朋友愿意去，\(x_2 = 0\)表示不愿意，同样的\(x_3\)来表示交通。</p>
<p>现在，假设你是奶酪的忠实粉丝，不管你的男(女)朋友是否有兴趣并且到那的交通非常不方便，你都很乐意前往。但是你是真的很难忍受坏天气，如果天气不好你说什么也不愿意去了。这时候你可以用感知器来建模做判断。一种可行的方式，选择\(w_1 = 6\)代表天气权重，\(w_2 = 2,w_3 = 2\)代表其他两项的权重。\(w_1\)的值越大说明你越重视天气因素，远远重于交通和男(女)朋友的态度。最后，假设你设定阀值5。根据这些选择，感知器变成了一个这样的需求决策模型，当天气好时模型输出1，天气不好的时候输出0，交通状况以及男(女)朋友的态度变得无关紧要。</p>
<p>通过改变权重和阀值，我们可以得到不同的决策模型。例如，假设将阀值设定为3，感知器会在天气好或者交通便利并且你男(女)朋友愿意陪你去的情况下告知你应该前往。换句话说，这是一个完全不一样的决策器。下调阀值意味着你前往的意愿在变大。</p>
<p>显然，感知器不是一个完全人性化的决策模型。但是它演示了感知器是如何通过加权证据并做出决策。我们似乎可以想象，一个复杂的由感知器组成的网络可以做一些相当微妙的决策：</p>
<p><img src="http://neuralnetworksanddeeplearning.com/images/tikz1.png" alt=""></p>
<p>在这个网络中，第一栏的感知器——通常称之为第一层感知器——完成三个很简单的决策，对输入信息进行加权。那么第二层的感知器呢？这一层的每个感知器对第一层的结果进行再次加权。如此，相对于第一层的感知器，第二层可以完成更复杂和更抽象的决策，然后再进一步复杂的决策由第三层感知器完成。这样一来，一个多层的感知器网络可以胜任复杂的决策。</p>
<p>顺带的，我在定义感知器的时候有说过，感知器只有一个输出。上面图片显示的网络中看上去是多个输出。实际上，它们仍然是一个输出，多个输出箭头表示的是一个输出被后续多个感知器当做输入。相比起来，画一个单个输出然后在拆分显得不简洁。</p>
<p>让我们来简化一下感知器的描述，条件表达式\(\sum_jw_j x_j &gt; \mbox{threshold}\) 显得繁琐，我们替换两个符号来达到简化的效果。首先，用点积替代\(\sum_jw_j x_j \)，\(w \cdot x \equiv \sum_j w_j x_j\)，\(w\),\(x\)分别表示权重和输入的向量。其次，将阀值移到不等式的左边来，用偏差概念替换原来的阀值概念：\(b \equiv -\mbox{threshold}\)。最后，感知器的规则表达式更新为：<br>$$<br>\begin{eqnarray}<br>  \mbox{output} = \left{<br>    \begin{array}{ll}<br>      0 &amp; \mbox{if } w\cdot x + b \leq 0 \<br>      1 &amp; \mbox{if } w\cdot x + b &gt; 0<br>    \end{array}<br>  \right.<br>\tag{2}\end{eqnarray}<br>$$<br>你可以把偏差理解成，感知器输出1的容易程度，或者使用更偏生物的术语，偏差是衡量感知器激活的容易度。当感知器拥有很大的偏差时，可以很轻易的得到1的输出；而当感知器的偏差趋向负无穷时，就很难得到1的输出了。显然，偏差的引入对感知器的描述来说只是一个小小的变化，但是后续我们可以感受到其带来大大的简化效果。所以，本书以后的内容中我们将不再使用阀值而使用偏差。</p>
<p>我已经描述了感知器如何通过了对证据加权来做决策。另外，感知器可以用于计算初级逻辑函数，比如我们通常认为的底层计算，“与”、“或”，“与非”。举个例子，假设我们有一个两输入感知器，权重皆为-2以及全局偏差3。如图：</p>
<p><img src="http://neuralnetworksanddeeplearning.com/images/tikz2.png" alt=""></p>
<p>于是，我们可以看到，输入00得到输出1(因为(−2)∗0+(−2)∗0+3=3(−2)∗0+(−2)∗0+3=3 为正，这里的*表示乘法运算)。同样的，输入01和10得到1，但是输入11则得到0((−2)∗1+(−2)∗1+3=−1(−2)∗1+(−2)∗1+3=−1 为负)。那么，我们的感知器变就成了一个“与非门”。</p>
<p>“与非”的例子显示我们可以利用感知器来计算简单的逻辑函数。实际上，我们可以使用感知器网络来计算任意的逻辑函数。原因在于“与非门”对计算是通用的，所以基于“与非门”可以实现任意的计算。例如，我们可以用与非门实现\(x_1,x_2\)求和的电路。这需要计算按位和，\(x_1 \oplus x_2\)，以及进位(当\(x_1,x_2\)都为1时进位为1)，进位即是按位积\(x_1x_2\)：</p>
<p><img src="http://neuralnetworksanddeeplearning.com/images/tikz3.png" alt=""></p>
<p>用权重为-2的二输入、全局偏差为3 的感知器替换与非门，就得到一个等价的感知器网络。下图就是该网络。注意到，为了方便的画出连线箭头，我把右下角的感知器稍稍移动了一下：</p>
<p><img src="http://neuralnetworksanddeeplearning.com/images/tikz4.png" alt=""></p>
<p>值得注意的一个地方是，最左边感知器的输出被最下面的感知器当做输入使用了两次。我在定义感知器的时候，并没有限定“双重输出到一个地方”是否可行。事实上，这问题不大，如果你要严格的限制这种情况，我们用一条权重为-4的连接线替换原来的两条权重为-2的连接线即可(如果对你来说不够明显，你可以稍微花点时间推算一下)。变换后的网络图如下(其他默认权重为-2，特别的将-4做了标记)：</p>
<p><img src="http://neuralnetworksanddeeplearning.com/images/tikz5.png" alt=""></p>
<p>到目前为止，我一直把输入变量\(x1,x2\)游离的画在最左边。通常情况会额外的加一层感知器(输入层)来对输入进行编码：</p>
<p><img src="http://neuralnetworksanddeeplearning.com/images/tikz6.png" alt=""></p>
<p>输入层的感知器符号(如下)，只有输出而没有输入：</p>
<p><img src="http://neuralnetworksanddeeplearning.com/images/tikz7.png" alt=""></p>
<p>这是一种简化，并不代表感知器真的没有输入。为了便于理解，假设确实这些感知器没有输入，那么加权求和\(\sum_j w_j x_j\)将一直是0，于是感知器只能输出固定的值：\(b&gt;0\) 输入1；\(b&lt;0\) 输入0，而不是我们所期望的值。最好的方式就是将输入感知器看做单纯用于输出期望的值\(x_1,x_2,\ldots\)的特殊单元。</p>
<p>加法器的例子展示了感知器网络如何模拟实现包含多个与非门的电路。因为与非门是通用的计算单位，所以感知器也遵循这个原则。</p>
<p>感知器的通用性让人感到兴奋的同时也让人失望。可喜的是，因为其通用性，感知器网络可以比得上其他任何计算机设备；而失望的是，感知器不就是另一种与非门吗？有什么可兴奋的呢。</p>
<p>然而，真实情况还不至于这么让人沮丧。事实证明，我们可以设计自动适配权重和偏差的学习算法用于网络中的神经元。这种适配源于对外部刺激的反馈，而不是由程序员的直接介入。这种学习算法给我们提供了一种完全不同于常规逻辑门的方法来使用人工神经元。不同于精确地将与非门的原件布局在电路上，神经网络是通过“学习”来解决的问题，因为有些时候要直接的设计常规的电路来应对某些难题会非常的困难。</p>
<h2 id="Sigmoid神经元"><a href="#Sigmoid神经元" class="headerlink" title="Sigmoid神经元"></a>Sigmoid神经元</h2><p>学习算法听起来妙极了。问题是我们如何才能设计出适用于神经网络的学习算法呢？假设我们有一个感知器网络，并且想用它来解决一些实际问题。例如，用手写体数字的原始像素扫面数据作为输入，我们期望这个网络能通过学习获得“权重”和“偏差”以便能正确的识别出扫面数据中的数字。为了演示这种学习是如何进行的，先假设我们对权重(偏差)做一点微小的改动，我们期望的是网络的输出也随之发生一点细微的变动，这种特性使得网络在某一时刻达到学习的目标。如下示意图展示了我们的期望(当然这个网络过于简单，不足以完成手写体的识别)：</p>
<p><img src="http://neuralnetworksanddeeplearning.com/images/tikz8.png" alt=""></p>
<p>如果权重(偏差)微小的变化只引起输出细微的变化，那我们可以依此来调整权重(偏差)的得到我们期望行为的网络。例如，假设我们的网络错误的将“9”识别成了“8”，我们可以找出如何在权重(偏差)上做微调让网络分类结果更接近“9”，然后不断的重复这种微调让输出结果越来越好，最终我们的网络学会了学习。</p>
<p>问题是，当我们的网络包含感知器的时候就不是这样的了。实际情况是，网络中任意一个感知器的权重或偏差发生一点细微的变化可能导致该感知器的输出翻转即0变1。这种翻转可能引起整个网络发生复杂的变化。所以，当你通过调整能正确的识别“9”了，但是其他的图片识别效果可能变得不可控了。这样一来，想要通过逐步的细微调整来让网络慢慢接近预期行为就变得不太现实了。也许确实存在某种巧妙的方法可以解决这种问题，但现在当务之急是我们怎么才能让感知器网络能学习。</p>
<p>我们引入另一种人工神经元来克服这个难题，也就是sigmod神经元。sigmoid神经元与感知器相似，但它在权重或偏差发生细微变化时只会引起输出的细微变化，这就是sigmoid网络能学习的关键因素。</p>
<p>现在，让我来正式描述一下sigmoid神经元。我们使用与感知器相似的图例表描绘sigmoid神经元：</p>
<p><img src="http://neuralnetworksanddeeplearning.com/images/tikz9.png" alt=""></p>
<p>跟感知器一样，sigmoid神经元有输入，\(x_1,x_2,\ldots\)，但是它的输出是0到1之间的任意实数，包含0和1，而不仅仅是0和1。比如，0.638…对sigmoid神经元来说是一个合法的输出。同样的，sigmoid神经元的每个输入也有权重，\(w_1,w_2,\ldots\)，以及全局偏差b。但是它的输出是\(\sigma(w \cdot x+b)\)，\(\sigma\)即是sigmoid函数，定义如下：<br>$$<br>\begin{eqnarray}<br>  \sigma(z) \equiv \frac{1}{1+e^{-z}}.<br>\tag{3}\end{eqnarray}<br>$$<br>我们把输入、权重以及偏差带入公式得到更精确的表达式：<br>$$<br>\begin{eqnarray}<br>  \frac{1}{1+\exp(-\sum_j w_j x_j-b)}.<br>\tag{4}\end{eqnarray}<br>$$<br>咋一看，sigmoid神经元与感知器差别很大。如果你不熟悉，sigmoid函数的代数表达式则显得晦涩难懂让人生畏。实际上，感知器与sigmoid神经元存在很多相似的地方，同时sigmoid函数的代数表达式更多的是显示了技术细节，而不是增加了理解上的障碍。</p>
<p>为了类感知器模型，假设\(z\equiv w \cdot x + b\)是一个非常大的正数，那么\(e^{-z}\approx 0\)，即\(\sigma(z) \approx 1\)。也就是说，当\(z = w\cdot x+b\)趋近于正无穷时，sigmoid神经元输出趋近于1，就像感知器。同理，当\(z = w \cdot x+b\)趋向于负无穷时，有\(e^{-z} \rightarrow \infty, \sigma(z) \approx 0\)。所以当\(z = w \cdot x +b\)趋向于负无穷时，sigmoid神经元也相当于一个感知器。仅当\(w \cdot x+b\)的值大小适中时，sigmoid神经元与感知器模型相差甚远。</p>
<p>\(\sigma\)的代数形式究竟是什么？我如何来理解它？其实，\(\sigma\)的精确形式并没有那么重要，真正重要的是函数画出来的图形，如下图：</p>
<p><img src="/img/ML/sigmoidfunction-1.png" alt=""></p>
<p>这是跃阶函数的平滑版图形：</p>
<p><img src="/img/ML/stepfunction-1.png" alt=""></p>
<p>如果\(\sigma\)变成跃阶函数，神经元的输出是0是1取决于\(w\cdot x+b\)是正还是负，那么sigmoid神经元则退化成了一个感知器。通过使用真实的\(\sigma\)函数我们就得到了一个平滑输出的感知器。实际上，真正关键的是\(\sigma\)函数的平滑度，而并非其形式的细节。平滑意味着权重和偏差的细微变化(\(\Delta w_j,\Delta b\))将引起输出的细微变化\(\Delta \mbox{output}\)。事实上，微积分告诉我们\(\Delta \mbox{output}\)约等于：<br>$$<br>\begin{eqnarray}<br>  \Delta \mbox{output} \approx \sum_j \frac{\partial \, \mbox{output}}{\partial w_j}<br>  \Delta w_j + \frac{\partial \, \mbox{output}}{\partial b} \Delta b,<br>\tag{5}\end{eqnarray}<br>$$<br>其中求和表示包括了所有的权重，\(\partial \,\mbox{output} / \partial w_j ,\partial \,\mbox{output} / \partial b\)分别为输出对于权重和偏差的偏导数。如果你不熟悉偏导数问题也不大。上面的表达式看上去很复杂，其实它揭示着很简单的东西(好消息)：\(\Delta \mbox{output}\)是关于权重和偏差变化(\(\Delta w_j,\Delta b\))的线性函数。这种线性特性让通过选择权重和偏差的细微变化得到期望的输出细微变化变得简单了。所以，sigmoid神经元在保留很多感知器属性的同时，也使得找出如何通过改变权重和方差来改变输出变得容易了很多。</p>
<p>如果真如上面说的，重要的是\(\sigma\)的形状而不是其精确形式，那我们为什么挑选的是公式(3)那样的表达式呢？实际上，本书后续内容中，我们会偶尔考虑其他形式激活函数\(f(\cdot)\)的神经元。当我们选取不同的激活函数时，主要是引起了偏导数(5)值的变化。后面会看到我们的偏导数计算由于选取了\(\sigma\)在代数上得到了很大的简化，原因在于指数函数在求导时有着很友好的特性。任何场景下，\(\sigma\)都是普遍应用于神经元中，这也是本书用到最多的激活函数。</p>
<p>我们如何解释sigmoid神经元的输出呢？显然，sigmoid神经元与感知器一个最大的区别在于sigmoid神经元的输出不仅仅是0和1，它可以输出0和1之间的任意实数，如0.173……和0.689……都是合法的输出。这是很有用的一个特性，比如，我们想用输出值来表示一个神经网络输入图像像素的平均强度。但也有麻烦的时候，假设，我们要求网络输出显示“输入图片是9”或者“输入图片不是9”。显然，像感知器那样直接输出1或0更容易。实际情况下，我们可以做一些约定来解决这个问题，如，当输出大于等于0.5时判定为“9“，小于0.5时判定不是”9“。为了不引起混淆，每次用到这种约定时我都会做明确的声明。</p>
<h3 id="练习"><a href="#练习" class="headerlink" title="练习"></a>练习</h3><ul>
<li><p><strong>Sigmoid神经元模拟感知器，一</strong></p>
<p>假设将网络中所有感知器的权重和偏差都乘以一个正的常数c。试证明网络的行为不会发生变化。</p>
</li>
<li><p><strong>Sigmoid神经元模拟感知器，二</strong></p>
<p>同样的，我们有一个感知器网络。</p>
</li>
</ul>
<h2 id="神经网络的结构"><a href="#神经网络的结构" class="headerlink" title="神经网络的结构"></a>神经网络的结构</h2><p>接下来一节中，我将介绍一个能很好的完成手写体数字分类的神经网络。我们先来了解一下定义网络中各个部分的术语。假设我们有一个如下图展示的神经网络：</p>
<p><img src="http://neuralnetworksanddeeplearning.com/images/tikz10.png" alt=""></p>
<p>如前面所提到的，神经网络中最左边的我们称之为输入层，对应的神经元则称之为输入神经元。最右边或者所谓的输出层包含了输出神经元，这里输出层只包含一个输出神经元。位于中间的，既不属于输入也不属于输出层的，我们称之为隐藏层。“隐藏”这个术语乍一听，让人有种神秘感，我头一回听到的时候以为这其中一定有很深的哲学含义或者数学思想。但实际上，它只是代表“既不是输入也不是输出”。上面展示的神经网络只有单一的隐藏层，而一些神经网络的隐藏层由多个层组成的。例如，下面的四层神经网络的隐藏层则有两层：</p>
<p><img src="http://neuralnetworksanddeeplearning.com/images/tikz11.png" alt=""></p>
<p>因为历史原因，这种多层的神经网络有时候也被称之为多层感知器或者MLPs，无论其中室友sigmoid神经元还是感知器构成。本书我不会使用MLP这个术语，以让读者困惑，这里只是特地说明有这回事，读者在以后碰到的时候能够分辨。</p>
<p>输入输出层通常设计得很简单。例如，假设我们要识别手写体的图像是否为“9”。一种通常的做法是在输入神经元对图像像素的强度进行编码，如果图像是64*64的灰度图像，则需要(4,096 = 64 \times 64)个输入神经元，用0到1之间合适的值表示其像素强度。输出层将只有一个神经元，当值小于0.5时判定“不是9”，当值大于0.5时判定“是9”。</p>
<p>隐藏层的设计则很具艺术性。尤其的，隐藏层的设计不是一些简单的法则可以概括的。相反的，神经网络研究人员为隐藏层开发了许多的启发式算法来帮助人们实现所期望的神经网络。如权衡隐藏层数与训练耗时的算法。本书后续会提到一些诸如此类的启发式算法。</p>
<p>截止到目前，我们讨论了前馈神经网络，即前一层的输出作为下一层的输入。神经网络中不存在环路，信息总是向前传播没有反向的。一旦引入环，则会出现(\theta)\函数的输入将受输出的影响的情况，这样的神经网络很难变得有意义，所以目前我们不考虑这种环路。</p>
<p>然而，在其他的人工神经网络模型中反馈回路是可行的，即<a href="https://en.wikipedia.org/wiki/Recurrent_neural_network" target="_blank" rel="external">RNN</a>。其思想是让神经元在一段有限的时间里处于激活状态，这些活跃的神经元可以刺激其他神经元在稍后变得活跃，继而刺激更多的神经元，一段时间后我们的到一个瀑布式的神经元激活流程。在这种模型中，回路不会引起问题，原因在于神经元的输出是在稍后才起作用，而不是即刻。</p>
<p>循环神经网络不如前向神经元网络应用广泛，部分原因在于循环神经网络的学习算法相对比较弱一些(至少目前为止)。但是循环神经网络依然很有意思，它的思想更接近人类的大脑工作模式。用循环神经网络来解决那些前向神经网络需要花费大力气才能解决的重大问题不是没有可能。然而，本书专注于应用更广泛的前向神经网络。</p>
<h2 id="一个简易的手写体数字识别神经网络"><a href="#一个简易的手写体数字识别神经网络" class="headerlink" title="一个简易的手写体数字识别神经网络"></a>一个简易的手写体数字识别神经网络</h2><p>在定义好神经网络后，我们回到手写体识别这个问题上。我将问题拆分为两个子问题。问题一，如何将包含多个数字的图片分割成只包含一个数字的小图，比如，我们要分割下面这个图片</p>
<p><img src="http://neuralnetworksanddeeplearning.com/images/digits.png" alt=""></p>
<p>分割成6个小图片，</p>
<p><img src="http://neuralnetworksanddeeplearning.com/images/digits_separate.png" alt=""></p>
<p>人工处理这种分割问题很简单，但是计算机要想准确的对图片做分割就很有挑战性了。但完成分割后，程序就需要对每个图片做分类。举个例子，我们用程序识别第一个数字，</p>
<p><img src="http://neuralnetworksanddeeplearning.com/images/mnist_first_digit.png" alt=""></p>
<p>这是一个5。</p>
<p>我们重点关注的是实现程序来解决问题二，即对每个独立的数字图片做分类。因为一旦有一个好的数字图片分类方法，图片分割就不是什么难事了。图片分割的方法非常多。一种可行的方法，先对图像做多种实验性分割，然后利用单个数字分类器对每类分割做评分。如果单个数字分类器对某种分割的所有分片都很有信心，则该分割方式获得一个高分；相反的，如果分类器对其中一个或多个分片的分类存在问题，则该分割方式获得低分。这种方法的核心思想是分割不准确讲导致分类器表现不佳。通过这种方式或者其衍生方法可以比较好的解决图片分割问题。所以，相比起图片分割问题，我们更关注能解决更多有趣且复杂问题的神经网络，即单个手写体数字识别。</p>
<p>我们将使用一个三层的神经网络来做单个数字识别：</p>
<p><img src="http://neuralnetworksanddeeplearning.com/images/tikz12.png" alt=""></p>
<p>输入层由对输入像素进行编码的神经元组成。如前面提到的，我们的训练数据是大量的28*28像素的手写体数字扫描图片，所以输入层需要(784 = 28\times 28)个神经元。出于简洁考虑，上面的图表没有完全画出784个神经元。输入像素是灰度值，0.0表示白色，1.0表示黑色，介于中间的随着值递增颜色从白逐渐加深变黑色。</p>
<p>第二层即是隐藏层。我们用变量(n)表示该层的神经元数目，(n)将取不同的值做测试。例图呈现了一个小型的隐藏层，只包含(n = 15)个神经元。</p>
<p>输出层则包含10个神经元。如果第一个神经元激活，即其输出(\approx 1)，那么神经网络判定输入数字是0，如果第二个神经元激活则判定是1，以此类推。更准确一点的说，我们将输出神经元从0到9编号，然后看哪个神经元有最高的激活值。比如编号6的神经元激活值最高，那么神经网络判定输入的数字是6，其他同理。</p>
<p>或许你会问为什么我们用了10个神经元。毕竟，神经网络的目标是识别出输入图片对应的是哪个数字((0, 1, 2, \ldots, 9))。更通常的做法4个神经元就够了，每个神经元可以表示两个值，根据其输出是接近0还是1。由于(2^4 = 16)，4个神经元足以表示10中可能性了，为什么我们还是用了10个神经元呢？这样不是效率更低吗？其实这是经验之谈，我们可以同时使用这两种设计，但最终在这个问题上，事实证明用10个输出神经元效果要好于用4个。那么问题来了，这是为什么呢？有没有什么启发式算法来告诉我们应该用10输出编码而不是用4输出编码呢？</p>
<p>为了理解为什么这么做，我们最好从本质上思考一下神经网络在做什么。考虑10个输出神经元的情形，我们只看第一个神经元，也就是通过权衡隐藏层的信息来判定是否是0的神经元。那么隐藏层做了什么事？根据输入简单的假设隐藏层中第一个神经元只是用于检测图像是否是下面这样的：</p>
<p><img src="http://neuralnetworksanddeeplearning.com/images/mnist_top_left_feature.png" alt=""></p>
<p>当输入像素与它匹配时增加其权重，对其他的输入则减轻其权重。同理，假设隐藏层中第二个、第三个、第四个神经元依次检测下列图像：</p>
<p><img src="http://neuralnetworksanddeeplearning.com/images/mnist_other_features.png" alt=""></p>
<p>那么，根据前面所看到的数字图片，你可以猜到这四个图片合起来就是一个0。</p>
<p><img src="http://neuralnetworksanddeeplearning.com/images/mnist_complete_zero.png" alt=""></p>
<p>所以，当隐藏层中这四个神经元都激活了，我们就可以得出结论，这数字是0。当然，这不是判断图像是否是0的的唯一方法，还有很多其他方法可以做到。但可以负责任的说，至少在这个例子中我们得出了输入是0的结论。</p>
<p>如果神经网络工作模式是这样的，那么我们似乎就可以解释为什么10输出网络要优于4输出网络了。如果我们只有4个输出神经元，那么第一个神经元需要判断输入图像更接近哪一个数字，但是要把上面那些简单的形状对应到某个数字就不是那么容易了。很难找出一种历史经验说明数字的局部形状与输出密切相关。</p>
<p>现在，说了这么多，都仅仅是一个引导性的。并不是说一个三层神经网络一定是像我说的那样运作的，即隐藏层的神经元只是检测简单的分量形状。或许有更聪明的算法能让我们使用4输出神经元。但是我通过这种探索性的思想可以把问题描述得很清晰，可以帮助你在设计一个好的神经网络中节省大量的时间。</p>
<h3 id="练习-1"><a href="#练习-1" class="headerlink" title="练习"></a>练习</h3><h2 id="用梯度下降进行学习"><a href="#用梯度下降进行学习" class="headerlink" title="用梯度下降进行学习"></a>用梯度下降进行学习</h2></div><script type="text/javascript" src="/js/share.js?v=0.0.0" async></script><a data-url="https://liamlee.github.io/2017/01/20/Chapter1_UsingNeuralNetsToRecognizeHandwrittenDigits/" data-id="ciyy3z9up0000lbs6v7ct7tpj" class="article-share-link">分享到</a><div class="tags"><a href="/tags/神经网络/">神经网络</a><a href="/tags/手写体识别/">手写体识别</a><a href="/tags/图像识别/">图像识别</a></div><div class="post-nav"><a href="/2017/02/09/logisticRegression/" class="pre">斯坦福(吴恩达)-机器学习-第三周编程作业代码笔记</a><a href="/2017/01/16/linearRegression/" class="next">斯坦福(吴恩达)-机器学习-第二周编程作业代码笔记</a></div><div id="disqus_thread"><script>var disqus_shortname = 'liamming1990';
var disqus_identifier = '2017/01/20/Chapter1_UsingNeuralNetsToRecognizeHandwrittenDigits/';
var disqus_title = '第一章 使用神经网络识别手写体数字';
var disqus_url = 'https://liamlee.github.io/2017/01/20/Chapter1_UsingNeuralNetsToRecognizeHandwrittenDigits/';
(function() {
  var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
  dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();</script><script id="dsq-count-scr" src="//liamming1990.disqus.com/count.js" async></script></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank" class="search-form"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="https://liamlee.github.io"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/总结/" style="font-size: 15px;">总结</a> <a href="/tags/神经网络/" style="font-size: 15px;">神经网络</a> <a href="/tags/图像识别/" style="font-size: 15px;">图像识别</a> <a href="/tags/机器学习/" style="font-size: 15px;">机器学习</a> <a href="/tags/线性回归/" style="font-size: 15px;">线性回归</a> <a href="/tags/LinearRegression/" style="font-size: 15px;">LinearRegression</a> <a href="/tags/LogisticRegression/" style="font-size: 15px;">LogisticRegression</a> <a href="/tags/手写体识别/" style="font-size: 15px;">手写体识别</a> <a href="/tags/东野圭吾/" style="font-size: 15px;">东野圭吾</a> <a href="/tags/读万卷书/" style="font-size: 15px;">读万卷书</a> <a href="/tags/Spark/" style="font-size: 15px;">Spark</a> <a href="/tags/大数据/" style="font-size: 15px;">大数据</a> <a href="/tags/安装教程/" style="font-size: 15px;">安装教程</a> <a href="/tags/Hue/" style="font-size: 15px;">Hue</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最新文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2017/02/09/logisticRegression/">斯坦福(吴恩达)-机器学习-第三周编程作业代码笔记</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/01/20/Chapter1_UsingNeuralNetsToRecognizeHandwrittenDigits/">第一章 使用神经网络识别手写体数字</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/01/16/linearRegression/">斯坦福(吴恩达)-机器学习-第二周编程作业代码笔记</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/12/29/look-back-2016/">回首2016</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/12/28/sparkNotebook-install/">Spark-Notebook安装与调教</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/12/26/Hue-Install/">Hue 安装配置</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/11/08/read-xianyirenXdexianshen/">读《嫌疑人X的献身》</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/10/19/hello-world/">Hello World</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-comment-o"> 最近评论</i></div><script type="text/javascript" src="//liamming1990.disqus.com/recent_comments_widget.js?num_items=5&amp;hide_avatars=1&amp;avatar_size=32&amp;excerpt_length=20&amp;hide_mods=1"></script></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">© <a href="/." rel="nofollow">Liamming.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a id="rocket" href="#top" class="show"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.pack.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="/css/jquery.fancybox.css?v=0.0.0"><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create','UA-86048320-1','auto');ga('send','pageview');
</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>